{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab2_TrainLeNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training our first Convolutional Neural Network\n",
        "\n",
        "With this notebook we are going to build and train our first Convolutional Neural Network (CNN). In particular, we will borrow the architecture proposed as [LeNet](https://ieeexplore.ieee.org/document/726791)."
      ],
      "metadata": {
        "id": "HVQuq23Q9Hyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1BimodSCOzNtpy76yE4QjJ5aCtsEzNNfX\" width=\"900\"></br></br>"
      ],
      "metadata": {
        "id": "accaY1dy-cs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start, as usual, by importing the necessary libraries."
      ],
      "metadata": {
        "id": "VtZwK8aDBxmK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIZU13zsxAM0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LeNet-5\n",
        "\n",
        "In order to build this model, we are going to need some **convolutional** and some **fully connected** layers. The former can be easily defined by exploiting the `torch.nn.Conv2D` module from PyTorch. Remember you can always take a look at the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)! We will also be using pooling operations (Max Pooling) to reduce the size of the feature maps. In particular, we are going to use the `torch.nn.functional.max_pool2d` module (details can be found, as usual, in the [docs](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)). Furthermore, for this model we are going to need the Rectified Linear Unit (ReLU) activation, available in the `torch.nn.functional.relu` module (details [here](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu))."
      ],
      "metadata": {
        "id": "TLLNs9dy9pAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet, self).__init__()\n",
        "    \n",
        "    # input channel = 1, output channels = 6, kernel size = 5\n",
        "    # input image size = (28, 28), image output size = (24, 24)\n",
        "    self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(5, 5))\n",
        "    \n",
        "    # input channel = 6, output channels = 16, kernel size = 5\n",
        "    # input image size = (12, 12), output image size = (8, 8)\n",
        "    self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5, 5))\n",
        "    \n",
        "    # input dim = 4 * 4 * 16 ( H x W x C), output dim = 120\n",
        "    self.fc3 = torch.nn.Linear(in_features=4 * 4 * 16, out_features=120)\n",
        "    \n",
        "    # input dim = 120, output dim = 84\n",
        "    self.fc4 = torch.nn.Linear(in_features=120, out_features=84)\n",
        "    \n",
        "    # input dim = 84, output dim = 10\n",
        "    self.fc5 = torch.nn.Linear(in_features=84, out_features=10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    \n",
        "    # first convolutional layer + relu\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    # Max Pooling with kernel size = 2\n",
        "    # output size = (12, 12)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "    \n",
        "    # second convolutional layer + relu\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    # Max Pooling with kernel size = 2\n",
        "    # output size = (4, 4)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "    \n",
        "    # flatten the feature maps into a long vector (-> (bs, 4*4*16))\n",
        "    x = x.view(x.shape[0], -1)\n",
        "\n",
        "    # first linear layer + relu\n",
        "    x = self.fc3(x)\n",
        "    x = F.relu(x)\n",
        "    \n",
        "    # second linear layer + relu\n",
        "    x = self.fc4(x)\n",
        "    x = F.relu(x)\n",
        "    \n",
        "    # output layer (linear)\n",
        "    x = self.fc5(x)\n",
        "    \n",
        "    return x"
      ],
      "metadata": {
        "id": "jtyBrLjJxNNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer & cost function\n",
        "We are going to use the familiar [Stochastic Gradient Descent (SGD)](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) optimizer and the [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) for our optimization."
      ],
      "metadata": {
        "id": "obE13C39_2g1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function\n",
        "\n",
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  return optimizer"
      ],
      "metadata": {
        "id": "JO_my8CC0Uhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and test steps\n",
        "We are going to implement our training and test pipelines as discussed in the previous lab sessions."
      ],
      "metadata": {
        "id": "qV2v8x0ZBG8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device='cuda'):\n",
        "\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # set the network to training mode\n",
        "  net.train() \n",
        "\n",
        "  # iterate over the training set\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "\n",
        "    # load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # loss computation\n",
        "    loss = cost_function(outputs,targets)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # parameters update\n",
        "    optimizer.step()\n",
        "    \n",
        "    # gradients reset\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # fetch prediction and loss value\n",
        "    samples += inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n",
        "\n",
        "    # compute training accuracy\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "def test_step(net, data_loader, cost_function, device='cuda'):\n",
        "\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # set the network to evaluation mode\n",
        "  net.eval() \n",
        "\n",
        "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # iterate over the test set\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      \n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # loss computation\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # fetch prediction and loss value\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "\n",
        "      # compute accuracy\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "nQmB7Kir8Y56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading\n",
        "In this block we are going to define our **data loading** utility. Differently from last time, in this case we are going to introduce **normalization**. This step is needed in order **bound** our values to the `[-1,1]` range, and obtain a **stable** training process for our network. This can be achieved by using the `torchvision.transforms.Normalize()` module (details [here](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html))."
      ],
      "metadata": {
        "id": "-xaALwsMD2vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(batch_size, test_batch_size=256):\n",
        "  \n",
        "  # prepare data transformations and then combine them sequentially\n",
        "  transform = list()\n",
        "  transform.append(T.ToTensor())                            # convert Numpy to Pytorch Tensor\n",
        "  transform.append(T.Normalize(mean=[0.5], std=[0.5]))      # normalize the Tensors between [-1, 1]\n",
        "  transform = T.Compose(transform)                          # compose the above transformations into one\n",
        "\n",
        "  # load data\n",
        "  full_training_data = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True) \n",
        "  test_data = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True) \n",
        "  \n",
        "\n",
        "  # create train and validation splits\n",
        "  num_samples = len(full_training_data)\n",
        "  training_samples = int(num_samples*0.5+1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "  # initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=4)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=4)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=4)\n",
        "  \n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "j1CzNOKhCe3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Put it all together!\n",
        "We are now ready to combine all the ingredients defined so far into our **training procedure**. We define a main function that **initializes** everything, **trains** the model over multiple epochs and **logs** the results."
      ],
      "metadata": {
        "id": "APuCISvfCrP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input arguments\n",
        "  batch_size: size of a mini-batch\n",
        "  device: GPU where you want to train your network\n",
        "  weight_decay: weight decay co-efficient for regularization of weights\n",
        "  momentum: momentum for SGD optimizer\n",
        "  epochs: number of epochs for training the network\n",
        "'''\n",
        "\n",
        "def main(batch_size=128, \n",
        "         device='cuda:0', \n",
        "         learning_rate=0.01, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=25):\n",
        "  \n",
        "  # get dataloaders\n",
        "  train_loader, val_loader, test_loader = get_data(batch_size)\n",
        "  \n",
        "  # instantiate model and send it to cuda device\n",
        "  net = LeNet().to(device)\n",
        "  \n",
        "  # instatiate optimizer and cost function\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  # run a single test step beforehand and print metrics\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  # iterate over the number of epochs\n",
        "  for e in range(epochs):\n",
        "\n",
        "    # train & log\n",
        "    train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "  # compute and print final metrics\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ],
      "metadata": {
        "id": "ur0qy-JBChFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run!"
      ],
      "metadata": {
        "id": "wGxHI0uxFCI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "hWKua57qE79H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KpqrkQQjGA-k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}