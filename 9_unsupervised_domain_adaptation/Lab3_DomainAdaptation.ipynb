{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab3_DomainAdaptation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Domain Adaptation\n",
        "In this notebook we will carry out a simple experiment of domain adaptation. As usual, let's start by importing the necessary libraries"
      ],
      "metadata": {
        "id": "Z2_IK6G_iUrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "import os"
      ],
      "metadata": {
        "id": "1kV7AhNIifan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network definition\n",
        "Each DA block consist of a domain specific Batch Normalization Layer followed by a domain agnostic scale-shift operation. Note that the domain specific BN layer will accumulate the domain specific first and second order statistics, i.e., mean and std. This can achieved be by setting `track_running_stats=True`. Details about this implementation can be found in the [docs](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html). We will employ the architecture presented in detail [here](http://proceedings.mlr.press/v37/ganin15-supp.pdf). "
      ],
      "metadata": {
        "id": "1jsJSefUijyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DIALNet(nn.Module):\n",
        "\n",
        "\tdef __init__(self):\n",
        "   \n",
        "\t\tsuper(DIALNet, self).__init__()\n",
        "  \n",
        "\t\tself.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
        "\t\tself.bns1 = nn.BatchNorm2d(64, affine=False)\n",
        "\t\tself.bnt1 = nn.BatchNorm2d(64, affine=False)\n",
        "\t\tself.gamma1 = nn.Parameter(torch.ones(64, 1, 1))\n",
        "\t\tself.beta1 = nn.Parameter(torch.zeros(64, 1, 1))\n",
        "\n",
        "\t\tself.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
        "\t\tself.bns2 = nn.BatchNorm2d(64, affine=False)\n",
        "\t\tself.bnt2 = nn.BatchNorm2d(64, affine=False)\n",
        "\t\tself.gamma2 = nn.Parameter(torch.ones(64, 1, 1))\n",
        "\t\tself.beta2 = nn.Parameter(torch.zeros(64, 1, 1))\n",
        "\n",
        "\t\tself.conv3 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
        "\t\tself.bns3 = nn.BatchNorm2d(128, affine=False)\n",
        "\t\tself.bnt3 = nn.BatchNorm2d(128, affine=False)\n",
        "\t\tself.gamma3 = nn.Parameter(torch.ones(128, 1, 1))\n",
        "\t\tself.beta3 = nn.Parameter(torch.zeros(128, 1, 1))\n",
        "\n",
        "\t\tself.fc4 = nn.Linear(6272, 3072)\n",
        "\t\tself.bns4 = nn.BatchNorm1d(3072, affine=False)\n",
        "\t\tself.bnt4 = nn.BatchNorm1d(3072, affine=False)\n",
        "\t\tself.gamma4 = nn.Parameter(torch.ones(1, 3072))\n",
        "\t\tself.beta4 = nn.Parameter(torch.zeros(1, 3072))\n",
        "\n",
        "\t\tself.fc5 = nn.Linear(3072, 2048)\n",
        "\t\tself.bns5 = nn.BatchNorm1d(2048, affine=False)\n",
        "\t\tself.bnt5 = nn.BatchNorm1d(2048, affine=False)\n",
        "\t\tself.gamma5 = nn.Parameter(torch.ones(1, 2048))\n",
        "\t\tself.beta5 = nn.Parameter(torch.zeros(1, 2048))\n",
        "\n",
        "\t\tself.fc6 = nn.Linear(2048, 10)\n",
        "\t\tself.bns6 = nn.BatchNorm1d(10, affine=False)\n",
        "\t\tself.bnt6 = nn.BatchNorm1d(10, affine=False)\n",
        "\t\tself.gamma6 = nn.Parameter(torch.ones(1, 10))\n",
        "\t\tself.beta6 = nn.Parameter(torch.zeros(1, 10))\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\n",
        "\t\tif self.training:\n",
        "\n",
        "\t\t\tx = self.conv1(x)\n",
        "\t\t\tx_source, x_target = torch.split(x, split_size_or_sections=x.shape[0] // 2, dim=0)\n",
        "\t\t\tx = F.max_pool2d(F.relu(torch.cat((self.bns1(x_source), self.bnt1(x_target)), dim=0)*self.gamma1 + self.beta1), \n",
        "                       kernel_size=3, stride=2)\n",
        "\n",
        "\t\t\tx = self.conv2(x)\n",
        "\t\t\tx_source, x_target = torch.split(x, split_size_or_sections=x.shape[0] // 2, dim=0)\n",
        "\t\t\tx = F.max_pool2d(F.relu(torch.cat((self.bns2(x_source), self.bnt2(x_target)), dim=0)*self.gamma2 + self.beta2), \n",
        "                       kernel_size=3, stride=2)\n",
        "\n",
        "\t\t\tx = self.conv3(x)\n",
        "\t\t\tx_source, x_target = torch.split(x, split_size_or_sections=x.shape[0] // 2, dim=0)\n",
        "\t\t\tx = F.relu(torch.cat((self.bns3(x_source), self.bnt3(x_target)), dim=0)*self.gamma3 + self.beta3)\n",
        "\n",
        "\t\t\tx = x.view(x.shape[0], -1)\n",
        "\t\t\tx = self.fc4(x)\n",
        "\t\t\tx_source, x_target = torch.split(x, split_size_or_sections=x.shape[0] // 2, dim=0)\n",
        "\t\t\tx = F.dropout(F.relu(torch.cat((self.bns4(x_source), self.bnt4(x_target)), dim=0)*self.gamma4 + self.beta4), \n",
        "                    training=self.training)\n",
        "\n",
        "\t\t\tx = self.fc5(x)\n",
        "\t\t\tx_source, x_target = torch.split(x, split_size_or_sections=x.shape[0] // 2, dim=0)\n",
        "\t\t\tx = F.dropout(F.relu(torch.cat((self.bns5(x_source), self.bnt5(x_target)), dim=0)*self.gamma5 + self.beta5), \n",
        "                    training=self.training)\n",
        "\n",
        "\t\t\tx = self.fc6(x)\n",
        "\t\t\tx_source, x_target = torch.split(x, split_size_or_sections=x.shape[0] // 2, dim=0)\n",
        "\t\t\tx = torch.cat((self.bns6(x_source), self.bnt6(x_target)), dim=0)*self.gamma6 + self.beta6\n",
        "\n",
        "\t\telse:\n",
        "\n",
        "\t\t\tx = self.conv1(x)\n",
        "\t\t\tx = F.max_pool2d(F.relu(self.bnt1(x)*self.gamma1 + self.beta1), kernel_size=3, stride=2)\n",
        "\n",
        "\t\t\tx = self.conv2(x)\n",
        "\t\t\tx = F.max_pool2d(F.relu(self.bnt2(x)*self.gamma2 + self.beta2), kernel_size=3, stride=2)\n",
        "\n",
        "\t\t\tx = self.conv3(x)\n",
        "\t\t\tx = F.relu(self.bnt3(x)*self.gamma3 + self.beta3)\n",
        "\n",
        "\t\t\tx = x.view(x.shape[0], -1)\n",
        "\t\t\tx = self.fc4(x)\n",
        "\t\t\tx = F.dropout(F.relu(self.bnt4(x)*self.gamma4 + self.beta4), training=self.training)\n",
        "\n",
        "\t\t\tx = self.fc5(x)\n",
        "\t\t\tx = F.dropout(F.relu(self.bnt5(x)*self.gamma5 + self.beta5), training=self.training)\n",
        "\n",
        "\t\t\tx = self.fc6(x)\n",
        "\t\t\tx = self.bnt6(x)*self.gamma6 + self.beta6\n",
        "      \n",
        "\t\treturn x"
      ],
      "metadata": {
        "id": "jpy7WudZihow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost function\n",
        "For the source domain, since we are provided with **label information**, we are can simply use the usual cross-entropy classification loss. On the other hand, we can define for the **unlabelled** target domain an entropy loss meant to maximally separate the unlabelled data. Please refer to the [original paper](https://arxiv.org/abs/1704.08082) for further details.\n",
        "\n",
        "$L^{t}(\\theta) = - \\frac{1}{m} \\displaystyle \\sum^{m}_{i=1} p_{i}\\log{p_i}$"
      ],
      "metadata": {
        "id": "d9SdCpippIre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ce_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function\n",
        "\n",
        "def get_entropy_loss(x):\n",
        "  p = F.softmax(x, dim=1)\n",
        "  q = F.log_softmax(x, dim=1)\n",
        "  b = p * q\n",
        "  b = -1.0 * b.sum(-1).mean()\n",
        "  return b"
      ],
      "metadata": {
        "id": "Fr58yPZEo0sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer\n",
        "We will employ, as usual, a stochastic gradient descent optimizer"
      ],
      "metadata": {
        "id": "0RAI5Ss1qZjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  return optimizer"
      ],
      "metadata": {
        "id": "R-Ch4knGqXjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and test steps\n",
        "We define our training and test steps for the DA experiment"
      ],
      "metadata": {
        "id": "ELB91PSFqwHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(net, source_data_loader, target_data_loader, optimizer, \n",
        "          get_ce_cost_function, entropy_loss_weight, device='cuda:0'):\n",
        "  source_samples = 0.\n",
        "  target_samples = 0.\n",
        "  cumulative_ce_loss = 0.\n",
        "  cumulative_en_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  \n",
        "  target_iter = iter(target_data_loader)\n",
        "\n",
        "  # strictly needed if network contains layers which has different behaviours between train and test\n",
        "  net.train()\n",
        "  for batch_idx, (inputs_source, targets) in enumerate(source_data_loader):\n",
        "    \n",
        "    # get target data. If the target iterator reaches the end, restart it\n",
        "    try:\n",
        "      inputs_target, _ = next(target_iter)\n",
        "    except:\n",
        "      target_iter = iter(target_data_loader)\n",
        "      inputs_target, _ = next(target_iter)\n",
        "    \n",
        "    inputs = torch.cat((inputs_source, inputs_target), dim=0)\n",
        "    \n",
        "    # load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # forward pass\n",
        "    outputs = net(inputs)\n",
        "    \n",
        "    # split the source and target outputs\n",
        "    source_output, target_output = torch.split(outputs, \n",
        "                                               split_size_or_sections=outputs.shape[0] // 2, \n",
        "                                               dim=0)\n",
        "    \n",
        "    # apply the losses\n",
        "    ce_loss = get_ce_cost_function(source_output,targets)\n",
        "    en_loss = get_entropy_loss(target_output)\n",
        "    \n",
        "    loss = ce_loss + entropy_loss_weight * en_loss\n",
        "    \n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # reset the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # print statistics\n",
        "    source_samples+=inputs_source.shape[0]\n",
        "    target_samples+=inputs_target.shape[0]\n",
        "    \n",
        "    cumulative_ce_loss += ce_loss.item()\n",
        "    cumulative_en_loss += en_loss.item()\n",
        "    _, predicted = source_output.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_ce_loss/source_samples, cumulative_en_loss/target_samples, cumulative_accuracy/source_samples*100\n",
        "\n",
        "\n",
        "def test_step(net, data_loader, cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # strictly needed if network contains layers which has different behaviours between train and test\n",
        "  net.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "\n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # print statistics\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "3LfI5IYhqdjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading\n",
        "In this block we define the data loading utility for our experiment"
      ],
      "metadata": {
        "id": "Fj0Oxms7rNkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(batch_size, test_batch_size=256):\n",
        "  \n",
        "  # prepare data transformations and then combine them sequentially\n",
        "  transform_mnist = list()\n",
        "  transform_mnist.append(T.ToTensor())                                              # convert Numpy to Pytorch Tensor\n",
        "  transform_mnist.append(T.Lambda(lambda x: F.pad(x, (2, 2, 2, 2), 'constant', 0))) # pad zeros to make MNIST 32 x 32\n",
        "  transform_mnist.append(T.Lambda(lambda x: x.repeat(3, 1, 1)))                     # to make MNIST RGB instead of grayscale\n",
        "  transform_mnist.append(T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]))    # normalizes the Tensors between [-1, 1]\n",
        "  transform_mnist = T.Compose(transform_mnist)                                      # composes the above transformations into one.\n",
        "  \n",
        "  transform_svhn = list()\n",
        "  transform_svhn.append(T.ToTensor())                                              # converts Numpy to Pytorch Tensor\n",
        "  transform_svhn.append(T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]))    # normalizes the Tensors between [-1, 1]\n",
        "  transform_svhn = T.Compose(transform_svhn)                                       # composes the above transformations into one.\n",
        "  \n",
        "  # load SVHN\n",
        "  source_training_data = torchvision.datasets.SVHN('./data/svhn', split='train', transform=transform_svhn, download=True) \n",
        "  \n",
        "  # load MNIST\n",
        "  target_training_data = torchvision.datasets.MNIST('./data/mnist', train=True, transform=transform_mnist, download=True) \n",
        "  target_test_data = torchvision.datasets.MNIST('./data/mnist', train=False, transform=transform_mnist, download=True)\n",
        "  \n",
        "  # initialize dataloaders\n",
        "  source_train_loader = torch.utils.data.DataLoader(source_training_data, batch_size, shuffle=True, drop_last=True)\n",
        "  target_train_loader = torch.utils.data.DataLoader(target_training_data, batch_size, shuffle=True, drop_last=True)\n",
        "  \n",
        "  target_test_loader = torch.utils.data.DataLoader(target_test_data, test_batch_size, shuffle=False)\n",
        "  \n",
        "  return source_train_loader, target_train_loader, target_test_loader"
      ],
      "metadata": {
        "id": "5M_6phJSrKmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Put everything together\n",
        "We are now ready to wrap everything up into our main function, where we initialize our components and loop over multiple epochs."
      ],
      "metadata": {
        "id": "utscxr0Vrma-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input arguments\n",
        "  batch_size: Size of a mini-batch\n",
        "  device: GPU where you want to train your network\n",
        "  weight_decay: Weight decay co-efficient for regularization of weights\n",
        "  momentum: Momentum for SGD optimizer\n",
        "  epochs: Number of epochs for training the network\n",
        "'''\n",
        "\n",
        "def main(batch_size=32, \n",
        "         device='cuda:0', \n",
        "         learning_rate=0.01, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50,\n",
        "         entropy_loss_weight=0.1):\n",
        "  \n",
        "  source_train_loader, target_train_loader, target_test_loader = get_data(batch_size)\n",
        "  \n",
        "  net = DIALNet().to(device)\n",
        "  \n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  cost_function = get_ce_cost_function()\n",
        "  \n",
        "  for e in range(epochs):\n",
        "    train_ce_loss, train_en_loss, train_accuracy = training_step(net=net, source_data_loader=source_train_loader, \n",
        "                                                         target_data_loader=target_train_loader, \n",
        "                                                         optimizer=optimizer, get_ce_cost_function=cost_function,\n",
        "                                                         entropy_loss_weight=entropy_loss_weight)\n",
        "    test_loss, test_accuracy = test_step(net, target_test_loader, cost_function)\n",
        "    \n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Train: CE loss {:.5f}, Entropy loss {:.5f}, Accuracy {:.2f}'.format(train_ce_loss, train_en_loss, train_accuracy))\n",
        "    print('\\t Test: CE loss {:.5f}, Accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')"
      ],
      "metadata": {
        "id": "fUu7L-eQrjon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's train!"
      ],
      "metadata": {
        "id": "dW8F5olCsLA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "YumOfgofsIpO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}