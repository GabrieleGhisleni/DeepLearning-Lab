{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4_CharRNN",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "Recurrent Neural Networks (RNN) are a family of models designed in order to model sequences of data (e.g. video, text). In this tutorial (adapted from [here](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)), we will see how to **predict the language of a name** using an RNN model, taking single word characters as input. \n",
        "\n",
        "Specifically, we will train the network on a list of surnames from 18 languages of origin, and predict which language a name is from based on the spelling:\n",
        "\n",
        "```\n",
        "$ python predict.py Hinton\n",
        "(0.63) Scottish\n",
        "(0.22) English\n",
        "(0.02) Irish\n",
        "\n",
        "$ python predict.py Schmidhuber\n",
        "(0.83) German\n",
        "(0.08) Czech\n",
        "(0.07) Dutch\n",
        "```"
      ],
      "metadata": {
        "id": "Z2_IK6G_iUrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading and preparing the data\n",
        "\n",
        "Let's start by downloading the zipped data provided in the original tutorial, and let's then extract it in our environment "
      ],
      "metadata": {
        "id": "E-W8XGEaNEwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "28yTDzpPM-RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the folder we can find 18 text files named as \"[Language].txt\". Each file contains a series of names, one name per line. In the following, we will take care of data preprocessing by:\n",
        "\n",
        "* extracting names and numbers of categories from the files\n",
        "* converting each name from Unicode to ASCII encoding\n",
        "* defining a dictionary containing all names (values) of a given language (key)"
      ],
      "metadata": {
        "id": "_KRCjjpWNYmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_filenames = glob.glob('data/names/*.txt')\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "# turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "print(unicode_to_ascii('Ślusàrski'))\n",
        "\n",
        "# build the category_lines dictionary\n",
        "# keys are the languages, and values are list of names for that language\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "# read a file and split into lines\n",
        "def readLines(filename):\n",
        "    lines = open(filename).read().strip().split('\\n')\n",
        "    return [unicode_to_ascii(line) for line in lines]\n",
        "\n",
        "for filename in all_filenames:\n",
        "    # extract the name of the language\n",
        "    category = filename.split('/')[-1].split('.')[0]\n",
        "    # read the names of that language\n",
        "    lines = readLines(filename)\n",
        "    # append to the list and add to the dictionary\n",
        "    all_categories.append(category)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)\n",
        "print('n_categories =', n_categories)"
      ],
      "metadata": {
        "id": "lkZHGCodNTW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding words into Tensors\n",
        "\n",
        "A crucial issue in this task lies in how how to define the input of the network. Since the network treats numbers - and not plain text - we must **convert text to numerical representation**. With this purpose, we represent each letter as a **one-hot vector** of size `(1, n_letters)`. A one-hot vector is filled with 0s, except for a 1 at the index of the current letter, e.g. `\"b\" = <0 1 0 0 0 ...>`.\n",
        "\n",
        "In order to build a word, we join these character representations into a 2D matrix `(line_length, 1, n_letters)`.\n",
        "\n",
        "That extra 1 dimension is due to the fact that PyTorch assumes the input to be divided in batches: we're just using a batch size of 1 here."
      ],
      "metadata": {
        "id": "m_kUmnhGOJVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "  \n",
        "# just for demonstration, encode a character into a (1, n_letters) tensor\n",
        "def letter_to_tensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    letter_index = all_letters.find(letter)\n",
        "    tensor[0, letter_index] = 1\n",
        "    return tensor\n",
        "\n",
        "\n",
        "# encode a line into a (line_length, n_letters) one-hot tensor,\n",
        "# (or (line_length, 1, n_letters) if the batch dimension is added)\n",
        "# a line is a sequence of characters\n",
        "def line_to_tensor(line, add_batch_dimension=True):\n",
        "    tensor = torch.zeros(len(line), n_letters)\n",
        "    for line_index, letter in enumerate(line):\n",
        "        letter_index = all_letters.find(letter)\n",
        "        tensor[line_index, letter_index] = 1\n",
        "\n",
        "    if add_batch_dimension:\n",
        "      tensor = tensor.unsqueeze(1)\n",
        "\n",
        "    return tensor\n",
        "  \n",
        "  \n",
        "# create a batch of samples given a list of lines\n",
        "# that is, a list of character sequences\n",
        "def create_batch(lines):\n",
        "    tensors = []\n",
        "    for current_line in lines:\n",
        "      # current_line_tensor is (line_length, n_letters)\n",
        "      current_line_tensor = line_to_tensor(current_line, add_batch_dimension=False)\n",
        "      tensors.append(current_line_tensor)\n",
        "    \n",
        "    # since each line_tensor may have a different line_length, we pad each\n",
        "    # line to the length of the longest sequence\n",
        "    padded_tensor = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=False, padding_value=0)\n",
        "    # padded_tensor is (max_line_length, batch_size, n_letters)\n",
        "    return padded_tensor"
      ],
      "metadata": {
        "id": "AEGRVRWROD9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Network\n",
        "\n",
        "We want to define a simple recurrent neural network. The newtork should have a recurrent layer followed by a fully connected layer mapping the features of the recurrent unit to the output space (i.e. number of categories).\n",
        "\n",
        "To run a step of this network, we need to provide an input (in our case, the tensor for the current sequence/s) and a previous hidden state (which we initialize as zeros at first). We'll get back the logits (i.e. network activation before the softmax) for each each language.\n"
      ],
      "metadata": {
        "id": "zP6MXSkWPTEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a simple recurrent network      \n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.i2h = nn.RNN(input_size, hidden_size)\n",
        "        self.i2o = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    # Forward the whole sequence at once\n",
        "    def forward(self, input, hidden=None):\n",
        "        if hidden==None:\n",
        "          hidden = self.init_hidden(input.shape[1])\n",
        "          \n",
        "        output, _ = self.i2h(input, hidden)\n",
        "        # only the features extracted at the end of the sequence are used\n",
        "        # to produce the output\n",
        "        output = self.i2o(output[-1])\n",
        "        \n",
        "        return output\n",
        "\n",
        "    # Instantiate the hidden state of the first element of the sequence dim: 1 x batch_size x hidden_size)\n",
        "    def init_hidden(self,shape=1):\n",
        "        return torch.zeros(1, shape, self.hidden_size)\n",
        "      \n",
        "      \n",
        "# create a simple LSTM network\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.i2h = nn.LSTM(input_size, hidden_size)\n",
        "        self.i2o = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, input, hidden=None, cell=None):\n",
        "        if hidden==None:\n",
        "          hidden = self.init_hidden(input.shape[1])\n",
        "          \n",
        "        if cell==None:\n",
        "          cell = self.init_hidden(input.shape[1])\n",
        "          \n",
        "        output, (_,_)= self.i2h(input, (hidden,cell))\n",
        "        # only the features extracted at the end of the sequence are used\n",
        "        # to produce the output\n",
        "        output = self.i2o(output[-1])\n",
        "        \n",
        "        return output\n",
        "\n",
        "    def init_hidden(self,shape=1):\n",
        "        return torch.zeros(1, shape, self.hidden_size)\n",
        "      \n",
        "    def init_cell(self,shape=1):\n",
        "        return torch.zeros(1, shape, self.hidden_size)\n",
        "      \n",
        "      \n",
        "# implement a simple RNN using cells\n",
        "class SimpleRNNwithCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleRNNwithCell, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.i2h = nn.RNNCell(input_size, hidden_size)\n",
        "        self.i2o = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden=None):\n",
        "        \n",
        "        if hidden==None:\n",
        "          hidden = self.init_hidden(input.shape[1])\n",
        "        \n",
        "        # manually feed each sequence element to the RNN cell\n",
        "        for i in range(input.shape[0]):\n",
        "          hidden = self.i2h(input[i],hidden)\n",
        "\n",
        "        output = self.i2o(hidden)\n",
        "          \n",
        "        return output\n",
        "\n",
        "    def init_hidden(self,shape=1):\n",
        "        return torch.zeros(shape, self.hidden_size)\n"
      ],
      "metadata": {
        "id": "ksAuhgIHPQNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing for training\n",
        "\n",
        "Before going into training we should make a few helper functions. The first one should interpret the output of the network, which we know to be a logits of each category. We can use `Tensor.topk` to get the index of the greatest value:"
      ],
      "metadata": {
        "id": "CbsIxX8dP_QD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def category_from_output(output):\n",
        "    # returns top_k_values, top_k_indices\n",
        "    top_values, top_idx = output.data.topk(1)\n",
        "    category_idx = top_idx[0][0]  # gets the index for the top value of the first batch element\n",
        "    return all_categories[category_idx], category_idx"
      ],
      "metadata": {
        "id": "yor-aotlP35k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want a quick way to get a training example (a name and its language):"
      ],
      "metadata": {
        "id": "Za7e14X3QRoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def random_training_pair(bs=1):\n",
        "    lines = []\n",
        "    categories = []\n",
        "\n",
        "    # each batch element is a random line from a random language\n",
        "    for b in range(bs):\n",
        "      category = random.choice(all_categories)\n",
        "      line = random.choice(category_lines[category])\n",
        "      \n",
        "      lines.append(line)\n",
        "      categories.append(category)\n",
        "      \n",
        "    # build the ground truth labels\n",
        "    categories_tensor = torch.LongTensor([all_categories.index(c) for c in categories])\n",
        "    # use our previous helper function to build the batch\n",
        "    # from a list of sequences of characters\n",
        "    lines_tensor = create_batch(lines)\n",
        "    \n",
        "    return categories_tensor, lines_tensor"
      ],
      "metadata": {
        "id": "3unsDbhFQO5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the network\n",
        "\n",
        "Now all it takes to train this network is showing it a bunch of examples, have it making guesses, and tell it if it's wrong.\n",
        "\n",
        "Since the output of the networks consists of logits - and the task is classification - we can use a standard cross-entropy loss."
      ],
      "metadata": {
        "id": "yw_GE4eEQzID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Nshn6EMTQjpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we instantiate a standard training loop where we will:\n",
        "\n",
        "*   forward the input to the network\n",
        "*   compute the loss\n",
        "*   perform backpropagation\n",
        "*   make a step with the optimizer\n",
        "*   reset the optimizer/network's grad"
      ],
      "metadata": {
        "id": "01HcYRBIQ_E7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(rnn, optimizer, categories_tensor, lines_tensor):\n",
        "\n",
        "    optimizer.zero_grad()    \n",
        "    output = rnn(lines_tensor)\n",
        "\n",
        "    loss = criterion(output, categories_tensor)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    return output, loss.item()"
      ],
      "metadata": {
        "id": "VGSQsBiuQ84E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just have to:\n",
        "* instatiate the network\n",
        "* instatiate the optimizer\n",
        "* run the training step for a given number of iterations"
      ],
      "metadata": {
        "id": "FQyKSa1TRPuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the network:\n",
        "n_hidden = 128\n",
        "rnn = SimpleRNN(n_letters, n_hidden, n_categories)\n",
        "\n",
        "# initialize the optimizer\n",
        "learning_rate = 0.005 # Example: different LR could work better\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
        "\n",
        "# initialize the training loop\n",
        "batch_size = 2\n",
        "n_iterations = 100000\n",
        "print_every = 5000\n",
        "\n",
        "# keep track of the losse\n",
        "current_loss = 0\n",
        "\n",
        "for iter in range(1, n_iterations + 1):\n",
        "    # get a random training input and target\n",
        "    category_tensor, line_tensor = random_training_pair(bs=batch_size)\n",
        "    \n",
        "    # perform the training step\n",
        "    output, loss = train(rnn, optimizer, category_tensor, line_tensor)\n",
        "    \n",
        "    # accumulate loss for printing\n",
        "    current_loss += loss\n",
        "    \n",
        "    # print iteration number and loss\n",
        "    if iter % print_every == 0:\n",
        "        print('%d %d%% %.4f ' % (iter, iter / n_iterations * 100, current_loss/print_every))\n",
        "        current_loss = 0\n"
      ],
      "metadata": {
        "id": "um1msplGRN5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying it out\n",
        "\n",
        "Finally, following the original tutorial [in the Practical PyTorch repo](https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification) we instantiate a prediction function and test on some user defined inputs."
      ],
      "metadata": {
        "id": "rfwWdBVfSUv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "def predict(input_line, n_predictions=3):\n",
        "    print('\\n> %s' % input_line)\n",
        "    output = rnn(line_to_tensor(input_line))\n",
        "    output = normalizer(output)\n",
        "    # get top N categories\n",
        "    top_values, top_index = output.data.topk(n_predictions, 1, True)\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(n_predictions):\n",
        "        value = top_values[0][i]  # 0 indexes the first batch element\n",
        "        category_index = top_index[0][i]\n",
        "        print('(%.2f) %s' % (value, all_categories[category_index]))\n",
        "        predictions.append([value, all_categories[category_index]])\n",
        "\n",
        "predict('Dovesky')\n",
        "predict('Jackson')\n",
        "predict('Satoshi')\n"
      ],
      "metadata": {
        "id": "F673kuLHRkx8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}