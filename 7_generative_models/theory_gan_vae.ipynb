{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models\n",
    "Generative model works in a context of unsupervised learning. in this scenario we do not have labels but we just have data. the main goal is to learn some underlying hidden structure of the data.\n",
    "\n",
    "One important task in unsupervised learning is **density estimation**, we want to learn the underlying probability model that generate this data. We want to do that because we want to sample from that model and produce data that are similar in my training set (no need of labels).\n",
    "\n",
    "the common tasks in unsupervised deep learning are:\n",
    "\n",
    "1. dimensionality reduction\n",
    "2. density estimation\n",
    "3. clustering\n",
    "\n",
    "One common taxonomy divided the models into non-probabilistic models (as auto encoders) and probabilist generative models, in this last category we have two possible approaches:\n",
    "\n",
    "Trying explicitly to learn the distribution of the data or use a smart strategy and use an implicit density estimation .\n",
    "\n",
    "This model rely only on data, given a huge dataset we want to generate new sample from the same distribution. Given a dataset of faces i want to generate a face. An application could be given training data, generate new samples from same distribution where we know that $training \\ data \\sim \\ p_{data(x)}$ and $p_{data(x)}$ is unknown we want to learn some $p_{model(x)}$ similar to  $p_{data(x)}$ where $p_{model(x)}$  is the distribution generated from the model.\n",
    "\n",
    "For doing that we have two main strategies:\n",
    "\n",
    "1. **Explicit density estimation**  where we explicitly define and solve for $p_{model(x)}$ figuring out an optimization approach such that the parameters are learned to get close to the $p_{data(x)}$ (which we do not have).\n",
    "\n",
    "1. **Implicit density estimation**   learn model that can sample from $p_{model(x)}$ without explicitly defining it. I want to learn a model that can sample from the data but i do not need a explicit function for it.\n",
    "\n",
    "![taxonomy_gan](images/taxonomy.png)\n",
    "\n",
    "With GAN we can generate images from sample but also art works, translate image domain from another, complex text into image, generate images and so on.\n",
    "\n",
    "Generative models can be used for simulation and planning (e.g. transform synthetic data into realistic ones) they also can also enable inference of **latent representations** that can be useful as general features.\n",
    "\n",
    "## Implicit density model\n",
    "### Generative Adversarial Networks (GAN)\n",
    "Generative Adversarial Networks (GANs) **do not consider any explicit density function**, they  just focus on the ability of the model to sample from a complex high-dimensional training distribution. \n",
    "\n",
    "Instead of sampling directly from image is to introducing a nn that sample such that you sample from a simple gaussian distribution and this will be passed to the generation network that has the role to translate this simple space in something complex that should resemble the data that we have. \n",
    "\n",
    "To ensure that the generation is done well we introduce another nn that will try to distinguish between real (from the training dataset) and fake images (generated).\n",
    "\n",
    "These family of models are called **sample generation**. The core idea is to sample from a very simple distribution, usually a normal distribution and learn transformation to training distribution with neural network.\n",
    "\n",
    "We pass a matrix of vector noise to our network generator abd this has the role to generate the random noise into something complex that should resemble the original dataset probability. this is done trough a **two-player game** where we have two neural nets one is the discriminator which will try to distinguish between real and fake images and a generator that try to fool the discriminator by generating realistic images.\n",
    "\n",
    "![gan](images/gan.png)\n",
    "\n",
    "The discriminator will be trained to understand if an image is real or fake while the generator will be pushed to generate image that seems more real to the discriminator.\n",
    "\n",
    "to train a han we need to train jointly the generator and the discriminator using a **min max objective function called adversarial objective**.\n",
    "\n",
    "I have two nn so i want to maximize to respect of the parameter of the discriminator say respect to the real data $E {x} p_{data}$; The second expectation is taken from the sample data $E {x} p_{z}$ because we are passing trough the generator; the discriminator is evaluating the output of the generator:\n",
    "\n",
    "1. The discriminator maximize such that the probability is close to 1 when i have real data and 0 when is fake data\n",
    "\n",
    "2. the generator is the opponent and aim to let the discriminator output one when the data are fake. \n",
    "\n",
    "![gan_loss](images/gan_loss.png)\n",
    "\n",
    "where $z$ is the noise vectors and $G(z)$ is the output of the generator given the noise vector $z$; $D(G(Z))$ is output of the discriminator when given fake generated data or $G(Z)$ and $D(X)$ is the output of the discriminator when given real training data from X.\n",
    "\n",
    "training gan is very hard, using this naive approach in practice does not work well. When sample is likely fake, we want to improve the generator but the gradient in that case s relatively flat (the model will learn only if it is already good enough to challenge the discriminator).\n",
    "\n",
    "The generator tries to minimize this function while the discriminator tries to maximize it. Looking at it as a min-max game, this formulation of the loss seemed effective.\n",
    "\n",
    "The Standard GAN loss function can further be categorized into two parts: Discriminator loss and Generator loss. $log(D(x))$ refers to the probability that the generator is rightly classifying the real image, maximizing $log(1-D(G(z)))$ would help it to correctly label the fake image that comes from the generator.\n",
    "\n",
    "The discriminator should maximize $Log(D(X))$, and as Log is a monotonic function so $Log(D(X))$ will automatically get maximized if the discriminator maximized $D(X)$. The discriminator needs to maximize $log(1 — D(G(Z)))$, which means it must have to minimize $D(G(Z))$.\n",
    "\n",
    "in the original paper the procedure that was proposed to train the gan we have gradient ascent on discriminator and gradient descent on generator, they realized that this introduce optimization difficulties.\n",
    "\n",
    "In practice, it saturates for the generator, meaning that the generator quite frequently stops training if it doesn’t catch up with the discriminator. We will have many update when the generator is already doing something good but at the beginning the gradient is flat so we are not improving much. \n",
    "\n",
    "#### Non-Saturating GAN Loss\n",
    "A subtle variation of the standard loss function is used where the generator maximizes the log of the discriminator probabilities $– log(D(G(z)))$.\n",
    "\n",
    "instead of performing gradient descent they perform gradient ascent also in the generator.\n",
    "\n",
    "This change is inspired by framing the problem from a different perspective, where the generator seeks to maximize the probability of images being real, instead of minimizing the probability of an image being fake. \n",
    "\n",
    "Anyway train two different nn is very difficult.\n",
    "\n",
    "#### Test loss does not imply quality images\n",
    "Another problem is that the learning curve is correlated on what the network is doing, while training GAN with this naive approach did not make the loss related to the quality of the images generated.\n",
    "\n",
    "Once the training is done we do not need the discriminator anymore, indeed we just keep the generator since we can produce new data from it. Another problem related to that is that the loss is not meaningful for understating if the quality of the generated image is good or not (in term of human understand).\n",
    "\n",
    "At test time the discriminator is not needed anymore, i just use it for training the generator; at test time i can sample from the simple distribution only the generator.\n",
    "\n",
    "GAN are very flexible we can create generator and discriminator with different architectures (using lice fully connected layers or conv net). Also the design of the generator/discriminator is very important to create good models.\n",
    "\n",
    "### Gan zoo\n",
    "#### DCGAN\n",
    "one the first work after the GAN breakthrough, the first questions were related to the architecture to use. the DCGAN proposed some principles for designing convolutional architectures for GANs, first model for large resolution image generation. \n",
    "\n",
    "The generator was build with a bunch of transposed convolutional layer and used ReLU, batchnorm and Tanh as last layer. \n",
    "\n",
    "Discriminator architecture\n",
    "- No pooling, only strided convolutions\n",
    "- Use Leaky ReLU activations (sparse gradients cause problems for training)\n",
    "- Use only one FC layer before the soft-max output\n",
    "- Use batch normalization after most layers\n",
    "\n",
    "It sample from distribution trough transpose convolution it produces images.\n",
    "\n",
    "![dcgan](images/dcgan.png)\n",
    "\n",
    "This work as become some much important is because they understand something about the role of the **latent space**; \n",
    "\n",
    "If taking the mean of the latent code of different images and perform some arithmetic we produce some images that is able to change only some particular features. By simply doing arithmetic they were able to condition the result of the generated image (e.g man with glasses -man without glasses + woman without glasses = woman with glasses). \n",
    "\n",
    "The first was the vector arithmetic with faces. For example, a face of a smiling woman minus the face of a neutral woman plus the face of a neutral man resulted in the face of a smiling man. Specifically, the arithmetic **was performed on the points in the latent space** for the resulting faces. Actually on the average of multiple faces with a given characteristic, to provide a more robust result.\n",
    "\n",
    "They start playing with latent features, like performing a pose transformation by adding a \"turn\" vector. \n",
    "\n",
    "This prove that the latent space is someway organized.\n",
    "\n",
    "### Evaluation GAN\n",
    "the problem related to GAN are traditionally:\n",
    "- stability of the GAN \n",
    "- the choice of the architectures \n",
    "- lastly when we generate something we have the problem of evaluating the performance.\n",
    "\n",
    "This is again a problem, whenever we have a generation process then we need a way to evaluate it and since there is an implicit distribution we do not have a proper way to evaluate GAN. \n",
    "\n",
    "1. Human performance:\n",
    "Human that is asked if the image is real or not for a lot og image. \n",
    "\n",
    "2. Inception Score:\n",
    "Key idea: **generators should produce images with a variety of recognizable object classes**. What i would expect from a GAN? i want that the images are various and if i have different objects i'd like that the GAN generate all the objects not a small subsets of it. If i look at the single image i want that the object is well recognize.\n",
    "\n",
    "following two criteria: \n",
    "    1. A human, looking at each image, would be able to confidently determine what is in there (**saliency**). \n",
    "    2. A human, looking at a set of various images, would say that the set has lots of different objects (**diversity*).\n",
    "\n",
    "\n",
    "I will do this using distribution, the **inception score** (because inception was the good classifier at the time). Probability that the object is in the image and the marginal distribution considering the labels.\n",
    "\n",
    "this classification is done using an image classifier, the main disadvantage is that a GAN that simply memorized the training data, if the GAN learn to copy the training set that already posses diversity and saliency (as it is) i'm not able to detect that the model is overfitting.\n",
    "\n",
    "The second problem is that if the GAN will outputs a single image per class (mode dropping) could still score well.\n",
    "\n",
    "3. Fréchet Inception Distance:\n",
    "Let's compare the statistics of the feature generate by all the network in the real and generated images, if the network as learned similar feature at different levels it means that the GAN is doing a good jobs (comparing statistics in term of first and second order statistics metrics).\n",
    "    1. Pass generated samples through a classification network and compute activations for a chosen layer.\n",
    "\n",
    "    2. Estimate multivariate mean and covariance of activations, compute Fréchet distance to those of real data\n",
    "\n",
    "Advantages: correlated with visual quality of samples and human judgment, Disadvantage: cannot detect overfitting.\n",
    "\n",
    "### Gan Issues\n",
    "The two main problems with GAN are related to the **stability** of the training (is hard to manage the double training, also they are very sensitive to hyper parameter selection), and second the **generator loss does not correlate with sample quality**. \n",
    "\n",
    "Lastly there is a problem called **mode collapse** in which the model concentrate only on few specific modes (classes) causing the generator to modelling only a small subset of the training data. The generator is not able to capture globally on the whole dataset but it will focused only on a subsample. The network will generate image images that are similar to a mode but many of them will be excluded (visually means that the GAN generate similar images).\n",
    "\n",
    "Some of these issue has been solved.\n",
    "\n",
    "### Training Tricks\n",
    "#### Feature Matching\n",
    "for avoiding mode collapse we can implement this solution where we add a further loss for feature matching, expanding the goal from beating the opponent to **matching features in real images**.\n",
    "\n",
    "This new loss is an additional requirements imposing some similarity at features level of a network of the real data and the generated data, this intuitively we expand the objective of our gan to beat the opponent but also match features of the real data.\n",
    "\n",
    "#### Mini-batch Discrimination\n",
    "Compute the similarity of the image $x$ with images in the same batch. appending the similarity $o(x)$ on one of the dense layers in the discriminator to classify wether this image is real of generated, **if the mode starts to collapse, the similarity of generated images increases, the discriminator can use $o(x)$ to detect generated images and penalize the generator if mode is collapsing**.\n",
    "\n",
    "the term $o(x)$ is computed by an extra layer design that compute the similarity of the features learned by the network inside a mini batch because if i have a mode collapse i will have a very high similarity (all images will be similar if mode collapse).\n",
    "\n",
    "#### Virtual Batch Norm\n",
    "the batch-norm is problematic because sample at batch level introduce some correlation on the data in the batches and correlation when you are generating something is not the best because they will be similar.\n",
    "\n",
    "the issue with batch norm is that the generated images are not independent, the core idea here is to sample a reference batch before the training to compute the normalization parameters $\\mu, \\sigma$ and combine a reference batch with the current batch to compute the normalization parameters.\n",
    "\n",
    "we compute the reference statistics that flow trough the network instead of relying always to the statistics of the batch that we are processing. \n",
    "\n",
    "instead of computing on the fly the first and second order stats i can compute beforehand some form of reference statistics and i will use those inside the network.\n",
    "\n",
    "#### Label & label smoothing\n",
    "If i have the label information that would be great to use it. Can i use the generator to work better and the discriminator in be facilitated the task. We are not using the latent code but also the label in the generator and in the discriminator.\n",
    "\n",
    "make sure discriminator provides good gradients to the generator even when it can confidently reject all generator samples (e.g. smooth discriminator targets for “real” data, e.g., use target value 0.9 instead of 1 when training discriminator). Since it is typically that the discriminator becomes stronger quicker a dynamic is to use this label smoothing (Since the discriminator learn faster we can use .9 instead of 1 for target value).\n",
    "\n",
    "The objective change is a very few process, the formula is the same by now we have the conditional dependencies $p(x|y)$.\n",
    "\n",
    "\n",
    "#### Conditional GAN (CGAN)\n",
    "it use the same loss as the original GAN but it use label info (1-hot encoding) both in generator and discriminator. in the generator the labels are used as extension to the latent space.\n",
    "\n",
    "![cgan](images/cgan.png)\n",
    "\n",
    "the idea is to impose some conditioning factor such as a label to be able impose generation in a more controlled manner.\n",
    "\n",
    "#### InfoGAN\n",
    "Creative design of GAN in order to address slightly different task. the main idea of InfoGAN is that we use the GAN to understand if there are some groups inside the data.\n",
    "\n",
    "Can i have a GAN to understand if the data are in some classes? here the label are treated as latent factor and doing in a way that are consistent with the real label (MNIST eg). The latent code will be correlated with some group information, we impose the network to learn the latent code but also to learn information that are semantically related to the classes in the dataset.\n",
    "\n",
    "#### Wasserstein GAN\n",
    "For improving the gradient descent one measure improvement is this called Wasserstein GAIN whe a different difference measure is used. Different cost function, better gradients, more stable training. \n",
    "\n",
    "More important the objective function value is more meaningfully related to quality of the generator output (test loss related to the quality of the image).\n",
    "\n",
    "#### LSGAN\n",
    "We replace the objective function with a different, instead the log loss with least square losses for generator and discriminator allowing more stable and robust trainings. Better quality images and more robust to mode collapse.\n",
    "\n",
    "### Image2Image translation\n",
    "instead of using random noise can we use an image from a different domain? instead of random noise we have a transformation networks. \n",
    "\n",
    "What if i feed a segmentation mask, can i have a GAN that produces a realistic images that comply this segmentation? from landscape can i have a GAN that produces a map?\n",
    "\n",
    "#### Pix2Pix network\n",
    "The network is considering pair of images (as in sequence) from example a scene and a corresponding segmentation and the goal is to train the discriminator accordingly to classify if the pairs of scenes were true or not.\n",
    "\n",
    "Instead of having random noise i have a transformation network that is still trained in an adversarial network, the generator will be translate into an image of the output domain and the discriminator will try to understand which are the mode (from segmentation it generate the real image and the discriminator have to distinguish between the generated and real).\n",
    "\n",
    "This required a paired images but this is a limiting factor.\n",
    "\n",
    "#### CycleGAN\n",
    "complete unpaired image to image translation, the goal is to learn a **transformation across domain**. We do not have paired information we have a lot of images not paired.\n",
    "\n",
    "The idea is that here we have two generator and two discriminator that operate in their own distribution and learn a mapping to translate their distribution to the others.\n",
    "\n",
    "it is a GAN that operate in two directions, i have a generator a discriminator and i also have a reconstruction network. I want to translate the images of real photography to van gogh paintings.\n",
    "\n",
    "From real image the nn learn how to generate van gogh images (compare the generated van gogh with real van gogh) what is new is that another network that will try to reconstruct the original image.\n",
    "\n",
    "This is done in two directions and the two networks will share the parameters:\n",
    "\n",
    "![cyclegan](images/cycle_gan.PNG)\n",
    "\n",
    "#### Text-To-Image synthesis\n",
    "Can i condition the generator not only with visual information but also with text information? if together with noise i add some text information and impose the network to generate images that are associated to this text (the discriminator will do the same but with text description).\n",
    "\n",
    "I'm expanding the conditional gan with text instead of only labels.\n",
    "\n",
    "### GAN Summary\n",
    "\n",
    "- Gans do not consider an explicit density function\n",
    "- A game-theoretic approach is used; \n",
    "- the models learn how to generate sample from training data trough a two-player game (introducing the discriminator). \n",
    "\n",
    "the main problem is that is tricky and unstable to train and we do not have a proper way to check the results.\n",
    "\n",
    "## Non Probabilistic Approach\n",
    "### Autoencoders\n",
    "In some situation unsupervised learning are used to compress data, here i want to learn some non-linear transformation into a lower dimension while keeping the most important features.\n",
    "\n",
    "Autoencoders are very old, they were a technique for training a backbone for supervised learning task. The core idea is to encode information, compress information automatically. The compression is because i have a first network that compress the information and then a decoder that mirror the structure of the encoder and gets back to the original image. The best compact representation of the data is the one that allow to reconstruct the image.\n",
    "\n",
    "autoencoders is an unsupervised approach for **learning a lower-dimensional feature representation** from unlabeled training data.\n",
    "\n",
    "![ae](images/ae.png)\n",
    "\n",
    "the dimension of $z$ usually is smaller than $x$ since only the relevant features are kept and the noise is suppressed. We train such that features can be used to reconstruct original data “Autoencoding” - encoding itself.\n",
    "\n",
    "for implementing the encoders we can use some linear layer plus some non linear activation functions. Originally the used Linear Layers + sigmoid functions then start using CNN architectures for encoding the image into the latent feature and deconvolutional layer for recreating the image.\n",
    "\n",
    "At the beginning autoencoders were use to initialize a supervised model, basically they were used as feature extractor. \n",
    "\n",
    "![ae](images/ae_2.png)\n",
    "\n",
    "Similar to GAN once the training is over we discard the reconstructor and use only the encoder model.\n",
    "\n",
    "One popular variation is the denoising autoencoders, to force the auto-encoder to learn useful features is adding random noise to its inputs and making it recover the original noise-free data. \n",
    "\n",
    "## Explicit density model\n",
    "### AutoRegressive models\n",
    "the GAN does not try to learn a probabilistic model but instead the idea of auto-regressive model is to explicitly model the likelihood of each image and since each image is composed by pixel we would like to decompose the likelihood of an image into the product of the likelihood of each pixels (if i observe the pixel in position i it depends to others pixel, called context). \n",
    "\n",
    "Use chain rule to decompose likelihood of an image x into product of 1D distributions where the likelihood of image $x$ is the product of several probability condition, in particular the probability of pixel $I$ given the previous (called **context**).\n",
    "\n",
    "$$p_{\\theta}(x)= \\prod_{i=1}^{n} p(x_{i}| x_{1}, ... , x_{i-1})$$\n",
    "\n",
    "This approach turns the modeling problem into a sequence problem wherein the next pixel value is determined by all the previously generated pixel values. The problem of generation is addressed as problem of learning a sequence (sequence of pixel given the context).\n",
    "\n",
    "Which is the neural net that is the best for express this probability distributions, this require the decision of the network (recurrent or convnet) and the decision of the context.\n",
    "\n",
    "The main problems with this approach is that we have to define the ordering of previous pixels and most important since they predict one pixel at the time they are very slow. \n",
    "\n",
    "#### PixelRNN\n",
    "First approach tried was to use the LSTM for capturing the context, the models start from the corner and the dependencies are captured using a two-dimensional LSTM; Here we have to also consider the channel dependencies. The image is generated pixel by pixel.\n",
    "\n",
    "For solving the problem of conserving spatial information is using the diagonal BiLSTM in order to capturing the most meaningful context as possible. Diagonal because i need the spatial context not only temporal. More over the image is a tensor not a matrix so it was necessary to create dependencies among the different channels. \n",
    "\n",
    "The main advantage of this approach is that using an explicit density model we have a simple way to compare methods and results since we can compare directly the resulting distribution while the main drawback is that the are very slow (one pixel at the time) and the quality is low.\n",
    "\n",
    "#### PixelCNN\n",
    "they tried to address the slow sequential generation using a CNN rather than a recurrent nn. \n",
    "\n",
    "Still generate one pixel at the time starting from the cornel but in this case we use a convolutional nn rather than a RNN. Dependency on previous pixels now modeled using a CNN over context region. I can still address generation the pixel according to the context trough a usage of a mask.\n",
    "\n",
    "they still generate one pixel at the time but for the training phase we can parallelize the operation but at test time not.\n",
    "\n",
    "At each point we sill predicting a vector \\[0,255] representing the probabilities of the pixel value:\n",
    "\n",
    "![pixel_cnn](images/pixelcnn.png)\n",
    "\n",
    "Training is faster than PixelRNN (can parallelize convolutions since context region values known from training images) but at the test time it is still slow since it has to compute one pixel at the time.\n",
    "\n",
    "a little variation of pixel CNN is using multi-scale context where we do not use all the previous pixels but just a grid of them. This can be done with pixel cnn using the delation kernel.\n",
    "\n",
    "#### AutoRegressive advantages\n",
    "\n",
    "Pros: \n",
    "- Can explicitly compute likelihood p(x) \n",
    "- Explicit likelihood of training data gives good evaluation metric.\n",
    "- Good samples \n",
    "\n",
    "the main disadvantages regard the computation time and the assumptions made to capture the context.\n",
    "\n",
    "### Variation AutoEncoders (VAE)\n",
    "Can i take autoencoders and move to the probabilistic word?\n",
    "\n",
    "PixelCNNs define tractable density function, optimize likelihood of  training data:\n",
    "\n",
    "$$p_{\\theta}(x)= \\prod_{i=1}^{n} p(x_{i}| x_{1}, ... , x_{i-1})$$\n",
    "\n",
    "VAEs define in **tractable density function with latent $z$**:\n",
    "\n",
    "$$p_{\\theta}(x) = \\int p_{\\theta}(z) p_{\\theta}(x|z) dz$$\n",
    "\n",
    "where $p_{\\theta}(z)$ is the simple gaussian prior distribution of the latent features while $p_{\\theta}(x|z)$ is the decoder nn. they are defining the probabilities by marginalizing such a latent space.\n",
    "\n",
    "Cannot optimize directly, derive and optimize lower bound on likelihood instead; VAEs are combination of autoencoders with Variational approximation and variational lower bound. \n",
    "\n",
    "Autoencoder that operate on latent features and pas hem into a probabilistic view that allow to optimize the likelihood function.\n",
    "\n",
    "![vae](images/vae.png)\n",
    "\n",
    "in autoencoders there is nothing that force the latent space to be in a good probability distribution space, the main idea is to rephrase autoencoders with a probabilistic spin. \n",
    "\n",
    "We will have two models: a function represented by the encoder that produces the latent feature representation given the image $z|x$ and then we have the decoder that emulate the function $x$ given $z$. And we use the $p(z)$ a simple gaussian probability distribution, so the latent space will be forced to be similar to another distribution (such as a gaussian distribution). The difference is in this core probabilistic approach to the latent space.\n",
    "\n",
    "we want to give to our latent feature a probabilistic fashion and so we want the ability to sample from a probability distribution associated with our latent space. \n",
    "\n",
    "$z$ is the latent factor used to generate the images back, i'm interested to estimate the optimal parameters according to the maximum likelihood, in order to do this i need to define some choices (the prior distribution for $z$ is a gaussian).\n",
    "\n",
    "The main problem is that when learning the parameters to maximize the likelihood of the training data the optimization function is intractable (cannot compute $p(x|z)$ for every $z$). The object to maximize is intractable. \n",
    "\n",
    "at the end the main intuition is that i have a first neural network that model the first conditional distribution $p_{\\theta} (x|z)$ and then i will have another neural network that model the mirror conditional distribution $q_{\\phi} (z|x)$. \n",
    "\n",
    "This means that the encoder network is a nn that learn how to predict the mean and the covariant of the first distribution. while the decoder will be another neural network that produce the mean and the covariance for the second conditional distribution (since we have to sample we have to implement the reparametrization tricks for gradient descent).\n",
    "\n",
    "we then sample z with the mean and covariance estimated and reconstruct x from z with the mean anf covariance estimated trough the decoder. \n",
    "\n",
    "![vae](images/vae_2.png)\n",
    "\n",
    "Solution: In addition to decoder network modeling $p_{\\theta} (x|z)$, define additional encoder network $q_{\\phi} (z|x)$ that approximates $p_{\\theta} (z|x)$. This allows us to derive a lower bound on the data likelihood that is tractable, which we can optimize.\n",
    "\n",
    "$$log p_{theta}(x^{(i)}) = E_{z} [log p_{\\theta} (x^{(i)}|z)] - D_{KL} (q_{\\phi}(x^{(i)}|z)|| p_{\\theta}(z)) + D_{KL} (q_{\\phi}(x^{(i)}|z)|| p_{\\theta}(z|x^{(i)})) $$\n",
    "\n",
    "i want to maximize the llikelhood of my training point and at the end this likelihood is intractable, is the sum of three term and one of them in not possible to compute but since it is a kullbach divergence we know that is greater than zero.\n",
    "\n",
    "where $log p_{\\theta} (x^{(i)}|z)$ is the reconstruct of the input data somehow i want my nn to fit well the data (over the parameter of the decoder), the second KullbachLeiber divergence $- D_{KL} (q_{\\phi}(x^{(i)}|z)|| p_{\\theta}(z))$ is referred to make approximate posterior distribution close to the prior, it force the latent distribution similar to prior distribution (gaussian) and the last term is the intractable part that is always > 0. The loss is called **variational lower bound \"ELBO\"**. Here the two models go in the same direction, there is no adversarial fashion. \n",
    "\n",
    "this training involve sampling, to be able to do this we use the reparametrization trick.\n",
    "\n",
    "if i use normal autoencoders i will have some discontinuity in the latent space; how do i ensure if i taken a latent code i produce a meaningful reconstructed image? This is indeed real in VAE because the KullbachLeiber term push the distribution to be similar to a prior. I will have closer point in the latent space that are similar to a particular digit and i do not have discontinuity moving inside the latent space, if we interpolate from the latent space representing '1' and '2' in the middle i'll have something that seems to both while in normal autoencoders could not be the case. VAE allow to move smoothly in the latent feature producing different concepts.\n",
    "\n",
    "Once the model is trained we can use the decoder network to sample $z$ from a prior distribution and generating new images and we can think as the dimension of $z$ as feature of the images itself (like head pose for $z_{1}$ or degree of similarity $z_{2}$). Since we are learning a certain priori it means that we are learning independent latent variable that will be encoded with different degree of variation in the data. \n",
    "\n",
    "to sum up:\n",
    "VAE are a **Probabilistic spin** to traditional autoencoders in which a intractable density is defined and is derive and optimized indirectly trough a lower bound.\n",
    "\n",
    "with VAE we are able to generate complete new image by perturbating the latent feature space $z$. vae are a probabilistic twist on autoencoders, sample from the mean and sd to compute the latent space. we break down the latent space z into a mean vector and a standard deviation, the goal of the encoder is to output a mean and sd that correspond to the distribution of $z$ vector introducing an element of stocasticity.\n",
    "\n",
    "both encoder and decoder are probabilistic in nature, the encoder wll try to infer the probability distribution of the latent space respect to the data $q_{\\phi}(z|x)$ while the decoder is trying to infer a new probability distribution of the input given the latent distribution $p_{\\theta}(x|z)$.\n",
    "\n",
    "The loss is given by\n",
    "\n",
    "$$L(\\phi, \\theta, x) = reconstruction\\ loss + regularization\\ term$$\n",
    "\n",
    "where the reconstruction loss is given by typically L2 norm between the input image and the reconstructed image. while the regularization term: $D(q_{\\phi}(z|x) || p(z))$. The term $q_{\\phi}(z|x)$ is the computation that our encoder try to learn  (distribution of latent space given the data $x$) and what we do regularizing this network is to place a prior distribution (it means some initial hypothesis about the real distribution could look likes) and will help the network to create a structure that roughly follow this prior distribution to be. This prior is usually a gaussian distribution meaning that is center in mean with 0 and variance of 1, this means in practice to place the latent vectors in the center of this latent space. the kl divergence is a measure of how far are two distributions. \n",
    "\n",
    "Using a prior as a gaussian distribution allow us to have some desired properties:\n",
    "1. Continuity: if two points are similar in the latent space means that they will be similar after decoding.\n",
    "2. Completeness: when we sample from the latent space we want to obtain always meaningful content after decoding. \n",
    "\n",
    "how in practice we can train this network, end to end? by imposing this probabilistic we introduce stochasticity, during backprop we cannot propagate back since it is stochastic. The idea to solve this it to reparametrizing this sampling layer. The core idea is to more the stochasticity away frm the mean and sigma that we want to train basically keeping a fixed $\\mu$ and fixed $\\sigma$ and scale them bny a random constant. \n",
    "\n",
    "Vae with this notion of distribution over latent variable we can sample and perturb and tune the variable of feature keeping all the rest fixed and the output image change in the features (as head pose).\n",
    "\n",
    "### Conditional\n",
    "If we have labels we can use the conditional information and the loss function will change minimal.\n",
    "\n",
    "Conditional vae introduce the usage of the labels (or inputs in which we can condition the process) in the process, in this case twe just replace $p(x|z)$ with $p(x|z,y)$ and $q(x|z)$ with $q(x|z,y)$ and we go trough the same KL divergence procedure to the the same lower bound. \n",
    "\n",
    "The VAE was the first technique to perform the task of attribute-conditionated image generation, so they tried to embed a caption in an appropriate space and then enabling such a generation creating images according to the description.\n",
    "\n",
    "Basically we are modifying the latent feature with some kind of knowledge derived from the label.\n",
    "\n",
    "### VAE vs GAN\n",
    "some researcher to put together VAE and GAN and to create this combination model. in VAE is easy to find $z$ and we work addressing explicitly the likelihood but the problem is that the output blurry images. the VAE-GAN is to put together the best of both work, we will regenerator the images and then this image will be passed trough a real image to a discriminator.  \n",
    "\n",
    "\n",
    "## Final comparison\n",
    "there are three family of models:\n",
    "\n",
    "1. Autoregressive model: that allow a simple and stable training, they directly operate on the likelihood of the data. However they are highly inefficient (and do not provide any latent space)\n",
    "\n",
    "2. GAN: produces the sharpest images but the main difficulties is the unstable training dynamics.\n",
    "\n",
    "3. VAE: they are the best in order to map the data into a latent space (well organized) but tends to have blurry outputs. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
