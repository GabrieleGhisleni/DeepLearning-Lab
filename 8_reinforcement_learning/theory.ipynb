{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "it is a set of techniques that is been developed following the animal learning in a feedback like manner. The idea behind is that there is an agent that performs some actions and there is an environment that will provide some feedback to the agent in term of rewards and provide some information on the state.\n",
    "\n",
    "The goal is to provide a procedure to learn action in order to minimize some cumulative rewards. \n",
    "\n",
    "the environment provide feedback to the agent about it state at time t; We are going to see how to formalize mathematically this and how to learn policy on the best action that the agent should perform to maximize the cumulative rewards.\n",
    "\n",
    "the agent take action, the action affect the state and the agent receive some occasional reward that depend on the state (the reward is a function of the state).\n",
    "\n",
    "what occasional mean is very important, the learning schema is very different, we want to learn only when we receive some important feedback.\n",
    "\n",
    "We are interested in finding a policy:\n",
    "\n",
    "$$\\pi(s) \\rightarrow a$$\n",
    "\n",
    "we want to learn that maximize the expected cumulative reward over time. \n",
    "\n",
    "Reinforcement differ from supervised learning since is supervise learning we have pairs of data $x, y$; so you have a training set that represent a kind of teacher for the model. in supervised learning we are using the iid assumptions and we are reducing the loss trough gradient descent. Here for every step we have always supervision and another thing is that when we sample with iid we have no dependencies between the data which is completely the opposite in reinforcement learning.\n",
    "\n",
    "the learning in rl proceed thanks to the interaction between the agent and the environment, the evaluation of the action can come in **sparse way**. \n",
    "\n",
    "1. from state $s_{t}$ take action $a$ determined by policy $\\pi(s)$. \n",
    "\n",
    "2. Environment selects next state $s$ base on transition model $p(s |s,a)$ (markov model).\n",
    "\n",
    "3. Observe $s'$ and reward $r(s')$, update the policy.\n",
    "\n",
    "in this context each training step is not independent from the previous, the agent action determine the environment and determine the next observation. The rewards are signal that could be very sparse (not always observable) and the rewards and not differentiable functions so we cannot use the rewards as surrogate of the loss function.\n",
    "\n",
    "## Famous examples\n",
    "\n",
    "1. Atari games:\n",
    "    1. objective: complete the game with the highest score\n",
    "    2. state: raw pixel inputs of the game state\n",
    "    3. action: game control (left, right, ...)\n",
    "    4. reward: score increase or decrease\n",
    "\n",
    "this was the first approach mixing idea from traditional rl learning with deep neural network. \n",
    "\n",
    "2. Go:\n",
    "    1. objective: win the game\n",
    "    2. state: position of all pieces\n",
    "    3. action: where to put the next piece\n",
    "    4. reward: 1 if win at then of the game, 0 otherwise.\n",
    "\n",
    "this is a very difficult game since it has many possible combination. \n",
    "\n",
    "## Evolution of reinforcement learning\n",
    "the idea of having and agent interacting with the environment was already there before the deep learning revolution. the procedure that was adopted was to build a table that indicate how well a couple of state and action is doing and deriving a policy from such a table.\n",
    "\n",
    "The idea is to have a nn that provides indication to action looking at the state of the environment; deep network enable to learn when the set of possible actions and state grow too much.\n",
    "\n",
    "the learning loop seeing can actually derived from the markov decision processed framework, math framework that allow to formalize decision is a stochastic environment (from s to state s' is modelled with probability). We want to find a policy, which is a map that gives us all optimal actions on each state on our environment.\n",
    "\n",
    "in case of value function we will rely on bellman equation, you have an approach to compute the value function in an iterative efficient function and deriving the policy thanks to the bellman equation.\n",
    "\n",
    "markov decision process is a math formalize made of 5 elements:\n",
    "\n",
    "1. set of possible states\n",
    "2. set of possible actions\n",
    "3. distribution of reward\n",
    "4. transition probability: provide distribution over the next state based on the previous\n",
    "5. discount factor\n",
    "\n",
    "given the notation we can say that:\n",
    "\n",
    "1. we start from initial state $s_{0}$\n",
    "2. we perform actions $a$\n",
    "3. transition model, probability of state s' given s and a (markov assumption, depend on the previous state and the action so first order dependencies)\n",
    "4. reward\n",
    "5. policy\n",
    "\n",
    "the loop that we have seen before becomes now:\n",
    "\n",
    "1. at time step t=0, environment sample initial state $s_{0} \\sim p(s_{0})$, we have a prior distribution and we are sampling an initial state\n",
    "\n",
    "repeating:\n",
    "\n",
    "2. agent select and action\n",
    "3. environment sample reward\n",
    "4. environment sample next state\n",
    "5. agent receives the reward and the next state\n",
    "\n",
    "we are in a stochastic environment this is why we are generating samples. the goal is to optimal find the function that map a state to a function, the function that maximizes the cumulative discounted rewards. \n",
    "\n",
    "we have a stochastic model so we have probabilities for the action and the agent could pick not the highest probability. the policy is a map that tells the optimal action for every state.\n",
    "\n",
    "## Cumulative reward\n",
    "how to we formalize the cumulative reward? we are considering the reward for different time step, from the starting step to the last.\n",
    "\n",
    "$$R_{t} = \\sum_{t>0} r (s_{t}) $$\n",
    "\n",
    "for the fact that the actions could be infinite we introduce the discount factor (scalar between 0,1) and tells how much importance give to the future steps compared to the actual one(less or more importance).\n",
    "\n",
    "lower the discount the agent will maximize the actual rewards rather than the futures. \n",
    "\n",
    "## Q-function\n",
    "the toal reward, $R_{t} = r_{t} + \\gamma r_{t+1}$ is the discounted sum of all rewardfs obtained from the time $t$. The Q function is a function that takes as input the current state of the agent and the current agent of the agent in the current state and the output is the going to return the expected total future sum of rewards that the agent can receive after that actions (if we are taking a good action the q-value will be high.)\n",
    "\n",
    "$$Q(s_{t}, a_{t}) = E [R_{t}|s_{t}, a_{t}]$$\n",
    "\n",
    "how can we choose the action to take? we want to infer a policy $\\pi(s)$ where the goal is to infer the best action to take at its state $s$. the strategy is that the polic shoulde choose an action tha maximized the future rewards:\n",
    "\n",
    "$$\\pi^{*}(s)= argmax_{a} \\ Q(s,a)$$\n",
    "\n",
    "the action that result in the maximum Q value.\n",
    "\n",
    "## Value-based method\n",
    "the q-function is the foundation based on the value based method rl.\n",
    "How can we train a nn to learn the Q-function? we input only the state and we are going to learn and output the q-values for all the possible actions. \n",
    "\n",
    "![rl_2](images/rl_2.png)\n",
    "\n",
    "we are finding the $Q(s,a)$ and the action will be the $argmax_{a} \\ Q(s,a)$\n",
    "using this approach we are doing only one forward pass and having accessible all the possible Q-value for each actions. to train the agent we will ultimely maximize our target return.\n",
    "\n",
    "summary: we have a nn that takes the input at state t and output the Q value for n possible actions. Each output correspond to the q-value (expected return) of taking that action in this moment. we compute our policy looking at the q-value and taking the highest. When the agent take the action and the environment will respond with a NN and the process start again. \n",
    "\n",
    "the downside of Q-learning our model can only model scenario that we can define the action space in discrete and small pieces (cannot handle continoues action spaces).Policy is determinsitically computed from Q function by maximizing the reward (cannot earn stochastic policies).\n",
    "\n",
    "__\n",
    "\n",
    "we want an agent that optimize a function that is a specific value-function for Q(s,a); this approach implies the construction of a table and this will be the value of Q where are indicated the more efficient moves.\n",
    "\n",
    "gives the total amount of reward the agent can expect from a particular state to all possible states from that state. The value function is the expectation computed over the cumulative rewards where we start from an initial state and we have  policy that provide an action at and a transaction probability given the state and the action.\n",
    " \n",
    "in practice it is more common to use this Q-value function, that is a function of a state-action pair. given this function we can compute the policy. \n",
    "\n",
    "the optimal policy is given when we found our optimal Q-value function.\n",
    "\n",
    "how do we fill the table? we can use the dynamic programming procedure offered by bellman procedure. we have a procedure that allow to link the value function at state s and the same for q value function and the intuition is that if the ...\n",
    "\n",
    "traditional q-learning is core based that given a state and action i want to compute the q-value and being able to compute the table (that describes the best actions).\n",
    "\n",
    "the idea is to start building trough the feedback trough the environment will try to build the Q-learning matrix filled with the rewards. the whole point of Q-learning is that i given access by performing some experience when the agent is interacting with the environment and i want from an initialize zero matrix build the best possible matrix. and once i have that just use according to the state for maximize the policy. \n",
    "\n",
    "i will train the model trough episode. after many of them we have to combine them, by always choosing the highest q value.\n",
    "\n",
    "the difference with deep q-learning is that it enable to solve problems where the possible pairs of state-action are many. \n",
    "\n",
    "### Deep Q-Learning\n",
    "i want to learn the mapping, training a  depp nn to estimate Q-values. How do we train it? we do not have the possibility to have loss function.\n",
    "\n",
    "i want to develop a variation of q-learning by using a CNN (row pixel are my states). they developed an artificial agent that is an end-to-end reinforcement learning.\n",
    "\n",
    "they have a visual input and the goal is to create a single nn agent that is able to successfully being able to play as many game as possible (network and hyperparameters fixed for all games). \n",
    "\n",
    "Given a certain state (a configuration step) and the output is a Q values associated with state and action and all the possible actions (actions are few the state is very high). they stack raw pixel from last 4 frames.\n",
    "\n",
    "we need to train the nn and we do not have a naive way to design a loss function. the non naive way is trough the usage of the bellman equation. starting from it they use it to design a loss function that is somehow derived from training a nn such as to map a target function that is highly defined by the bellman equation that is moving (not fixed, they decided to keep if fixed for some steps and then change it progressively).\n",
    "\n",
    "the two problems related to this problem is the target function that is moving and the second is that the expected is computed on triplets but at every iteration this triplets are not highly iid and to solve that they use the experience replay. \n",
    "\n",
    "## Policy-based approaches\n",
    "in policy learning we are directly learning the policy that govern our ideal action step. It takes an input state and output an action. We are going to predicting not the Q-value but we are optimizing the policy $\\pi(s)$. The output gives the desired output and represent not the expected rewards but the probability that is a good action or not.\n",
    "\n",
    "If the output is the probability we can aggreagte them into our policy function and then we can sample from that action. This is now a distibution, every time you sample you are sampling from a distribution in a stocastich manner. Because of that the sum f the outputs should sum up to 1. \n",
    "\n",
    "\n",
    "![rl_3](images/rl_3.png)\n",
    "\n",
    "instead of learning Q(s,a) they try to understand the policy directly without an intermediate function.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
