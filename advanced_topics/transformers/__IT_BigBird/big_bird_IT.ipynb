{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "big_bird_IT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10Wyopuqs9zS",
        "outputId": "644415da-29cf-4704-e80c-4c1dd7767462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"drive/MyDrive/IT_big_bird\")\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Remember to restart the kernel after the installation!\n",
        "!pip install tokenizers\n",
        "!pip install transformers\n",
        "!pip install mlflow\n",
        "!pip install datasets\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "uF1kGmSdtIi5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load OSCAR Dataset\n",
        "\n",
        "We can download the OSCAR corpus dataset wichi contains around 90gb of data at the time. Since this is a very huge amount of data we can use the **streaming** parameter provided by the datasets library. Doing this operation we are able to stream the dataset without directly download all into our memory (in colab is limitated to 60gb)."
      ],
      "metadata": {
        "id": "0FrmRVDjWaAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset('oscar', \"unshuffled_deduplicated_it\", split='train',\n",
        "                                streaming=True, shuffled=True)\n",
        "\n",
        "dataset = dataset.with_format(\"torch\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3g793IlWU9c",
        "outputId": "338815ee-a215-422d-e0a2-4cd1c894b96d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration unshuffled_deduplicated_it-shuffled=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create new tokenizer\n",
        "\n",
        "To do this operation instead of creating a completly new BPE tokeizer with sentencepiece we can take advantage of \"train_new_from_iterator\" provide directly from huggingface tokenizer library. Doing this operation we are able to load previous pretrained tokenizer for the model that we are using (in this case one bigbird from google) and train a new one starting from it.\n",
        "\n",
        "The main advantage of this process is that the tokenizer comes with all the necessary configuration for that particular model that we want to train. Since the main goal is to create a BigBird for italian sentence we can keep them without changing any of the google provided configuration. "
      ],
      "metadata": {
        "id": "9pNTqW4OUBOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "old_tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n",
        "f\"vocab size from original bigbird {old_tokenizer.vocab_size}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Uz2RR1rtUuJg",
        "outputId": "90d55d9c-d3bc-4ddb-93ab-270db7c7a1cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'vocab size from original bigbird 50358'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are working with a streaming datasets wich is too big for our demostration purpose we can create an iterable function that allow us to limit the number of records that will be used for training our tokenizer.\n",
        "\n",
        "the function *train_new_from_iteration* takes an iteration which is the function that we've just create and the vocabulary size. in this case we are keeping the same from the original bigbird tokenizer. "
      ],
      "metadata": {
        "id": "BMeH_Ih9VgM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "\n",
        "LEN_TRAINIG = 2_000_000\n",
        "\n",
        "def get_training_corpus():\n",
        "    for idx, text in enumerate(dataset.take(LEN_TRAINIG)):\n",
        "      if (idx % 100_000 == 0): \n",
        "        print(idx, \"time: \", dt.datetime.now().strftime('%H:%M:%S'))\n",
        "      yield text[\"text\"]\n",
        "\n",
        "tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus(), old_tokenizer.vocab_size)\n",
        "tokenizer.save_pretrained('model/tokenizer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1fQOtY6IrIw",
        "outputId": "f79c0f40-efbe-40d6-9b16-5f0b6979290c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 time:  12:43:28\n",
            "100000 time:  12:48:06\n",
            "200000 time:  12:53:01\n",
            "300000 time:  12:58:23\n",
            "400000 time:  13:04:39\n",
            "500000 time:  13:09:48\n",
            "600000 time:  13:15:00\n",
            "700000 time:  13:19:59\n",
            "800000 time:  13:25:05\n",
            "900000 time:  13:29:26\n",
            "1000000 time:  13:35:35\n",
            "1100000 time:  13:39:43\n",
            "1200000 time:  13:44:28\n",
            "1300000 time:  13:48:38\n",
            "1400000 time:  13:53:15\n",
            "1500000 time:  13:57:32\n",
            "1600000 time:  14:01:23\n",
            "1700000 time:  14:06:24\n",
            "1800000 time:  14:10:40\n",
            "1900000 time:  14:16:07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now load and test how our new tokenizer behave."
      ],
      "metadata": {
        "id": "BDINi6FyW-i0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('model/tokenizer')\n",
        "\n",
        "for idx,val in enumerate(dataset):\n",
        "  tokens = tokenizer.encode_plus(val['text'])\n",
        "  print(tokens['input_ids'])\n",
        "  print(tokenizer.decode(tokens['input_ids']))\n",
        "  if idx > 10: break\n",
        "  "
      ],
      "metadata": {
        "id": "_JfBo_aVTTeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataCollator for Masking tokens\n",
        "\n",
        "Since we are training our model on the Masked Language task we have to mask some the tokens. Basically what we are doing consist of creating a trainining set where some word in the sentences will be randomly masked.\n",
        "\n",
        "In our case we use the default *mlm_probability* which is equal to 0.15, this means that the 15% of the tokens will be masked during the training process.\n",
        "\n",
        "Then we create the Torch DataLoader we recall that torch class Dataset stores the samples and their corresponding labels, while DataLoader wraps an iterable around the Dataset to enable easy access to the samples."
      ],
      "metadata": {
        "id": "kdnXFm25XUYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "FREQ_OF_MASKING = 0.15\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=FREQ_OF_MASKING)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, collate_fn=data_collator, batch_size=8)"
      ],
      "metadata": {
        "id": "TB7bQExPFAFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (BigBirdConfig, BigBirdForMaskedLM)\n",
        "\n",
        "\n",
        "\n",
        "config = BigBirdConfig()\n",
        "  \n",
        "model = BigBirdForMaskedLM(config=config)\n",
        "tokenizer = BigBirdTokenizerFast()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "JyOR19d_6NcE",
        "outputId": "5a7d1010-5a4e-4cce-b9c2-d2fa01add4ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-af559b6698a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigBirdForMaskedLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigBirdTokenizerFast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/tokenization_big_bird_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, tokenizer_file, unk_token, bos_token, eos_token, pad_token, sep_token, mask_token, cls_token, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mcls_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mmask_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         )\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             raise ValueError(\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0;34m\"Couldn't instantiate the backend tokenizer from one of: \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0;34m\"(1) a `tokenizers` library serialization file, \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;34m\"(2) a slow tokenizer instance to convert or \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    sent_mapping = {'neg':0, 'neu':1, 'pos':2}\n",
        "    def __init__(self, dataframe, tokenizer, max_len=140):\n",
        "        self._tokenizer = tokenizer\n",
        "        self.text = dataframe.text\n",
        "        self.targets = dataframe.sentiment\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    @property\n",
        "    def tokenizer(self):\n",
        "      return self._tokenizer\n",
        "\n",
        "    @property\n",
        "    def get_labels(self):\n",
        "      return self.labels\n",
        "\n",
        "    def pre_process_text(self,row):\n",
        "      row = re.sub(r'@\\S+|http\\S*|\\!|\\?|#|RT+|\\d+|\\.*|\\,*|\\-*|@|\\[|\\]|\\(|\\)|\\:|\\;|\\+|\\*|\\-|\\_|\\\"', '', row).strip()\n",
        "      row = row.lower()\n",
        "      return row\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        comment_text = self.text[index]\n",
        "        comment_text = self.pre_process_text(comment_text)\n",
        "        \n",
        "        inputs = self._tokenizer.encode_plus(\n",
        "            comment_text,\n",
        "            # None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_token_type_ids=False,\n",
        "        )\n",
        "\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return ({\n",
        "                'ids': torch.tensor(ids, dtype=torch.long),\n",
        "                'mask': torch.tensor(mask, dtype=torch.long),\n",
        "                'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "                })\n",
        "      \n",
        "\n",
        "\n",
        "def create_dataset(train_raw, test_raw, tokenizer, tokenizer_lenght, BATCH_SIZE):\n",
        "  training_set = CustomDataset(train_raw, tokenizer, tokenizer_lenght)\n",
        "  eval_set = CustomDataset(test_raw, tokenizer, tokenizer_lenght)\n",
        "  return (DataLoader(training_set, batch_size= BATCH_SIZE, shuffle=True), DataLoader(eval_set, batch_size= BATCH_SIZE, shuffle=True))"
      ],
      "metadata": {
        "id": "RgHhAIN1tOFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, model, loss_fn, epoch, path, optimizer, scheduler):\n",
        "    self.model = model\n",
        "    self.loss_fn = loss_fn\n",
        "    self.epochs = epoch \n",
        "    self.optimizer = optimizer\n",
        "    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
        "    self.scheduler = scheduler\n",
        "    self.path = path\n",
        "\n",
        "  def training_loop(self, training_loader, testing_loader, run_name, **kwargs):\n",
        "    self.model.to(self.device)\n",
        "    with mlflow.start_run(run_name=run_name) as run:  \n",
        "      for key, value in kwargs.items():\n",
        "        mlflow.log_param(key, value)\n",
        "      self.train(training_loader, testing_loader)\n",
        "      mlflow.pytorch.log_model(self.model, self.model.name)\n",
        "\n",
        "  def handle_metrics_multiclass(self, metrics, output, truth):\n",
        "    accuracy = accuracy_score(output.cpu(), truth.cpu())\n",
        "    batch_metric = classification_report(output.cpu(), truth.cpu(), \n",
        "                                         target_names=['neg','neu','pos'],\n",
        "                                         labels=[0, 1, 2], \n",
        "                                         output_dict=True, zero_division=0)\n",
        "    metrics['accuracy'] += accuracy\n",
        "    metrics['macro avg'][\"f1-score\"] += batch_metric['macro avg'][\"f1-score\"]\n",
        "    metrics['macro avg'][\"precision\"] += batch_metric['macro avg'][\"precision\"]\n",
        "    metrics['neg'][\"precision\"] += batch_metric['neg'][\"precision\"]\n",
        "    metrics['neg'][\"f1-score\"] += batch_metric['neg'][\"f1-score\"]\n",
        "    metrics['neg'][\"recall\"] += batch_metric['neg'][\"recall\"]\n",
        "    metrics['neu'][\"precision\"] += batch_metric['neu'][\"precision\"]\n",
        "    metrics['neu'][\"f1-score\"] += batch_metric['neu'][\"f1-score\"]\n",
        "    metrics['neu'][\"recall\"] += batch_metric['neu'][\"recall\"]\n",
        "    metrics['pos'][\"precision\"] += batch_metric['pos'][\"precision\"]\n",
        "    metrics['pos'][\"f1-score\"] += batch_metric['pos'][\"f1-score\"]\n",
        "    metrics['pos'][\"recall\"] += batch_metric['pos'][\"recall\"]\n",
        "\n",
        "    metrics['step'] += 1\n",
        "\n",
        "  \n",
        "  def log_multiple_metrics(self, metrics, step, prefix='train'):\n",
        "      for key, value in metrics.items():\n",
        "        if key != 'step':\n",
        "          if type(value) == dict:\n",
        "            for key_inside, value_inside in metrics[key].items():\n",
        "              mlflow.log_metric(f\"{prefix}_{key}_{key_inside}\", (value_inside/metrics['step']), step=step)\n",
        "          elif key == 'loss':\n",
        "           mlflow.log_metric(f\"{prefix}_{key}\", (value), step=step)\n",
        "          else:\n",
        "           mlflow.log_metric(f\"{prefix}_{key}\", (value/metrics['step']), step=step)\n",
        "\n",
        "      \n",
        "  def train(self, training_loader, testing_loader):\n",
        "        step = 0\n",
        "        for epoch in (range(self.epochs)):\n",
        "          self.model.train()\n",
        "\n",
        "          metrics_train = {\n",
        "              \"step\":0, \"accuracy\":0, \n",
        "              \"macro avg\": {\"f1-score\":0, \"precision\":0},\n",
        "              \"neg\": {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0},\n",
        "              \"neu\": {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0},\n",
        "              \"pos\": {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0}}\n",
        "\n",
        "          total, correct = 0,0\n",
        "          for idx, data in tqdm(enumerate(training_loader), total=len(training_loader)):\n",
        "              ids = data['ids'].to(self.device, dtype = torch.long)\n",
        "              mask = data['mask'].to(self.device, dtype = torch.long)\n",
        "              targets = data['targets'].to(self.device, dtype = torch.long)\n",
        "\n",
        "              outputs = self.model(ids, mask) \n",
        "              self.optimizer.zero_grad()\n",
        "              loss = self.loss_fn(outputs, targets)\n",
        "\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += targets.size(0)\n",
        "              correct += (predicted == targets).sum().item()\n",
        "\n",
        "              self.handle_metrics_multiclass(metrics_train, predicted, targets)\n",
        "\n",
        "              if idx%1000 == 0:\n",
        "                  metrics_train['loss'] = loss.item()\n",
        "                  self.log_multiple_metrics(metrics_train, step=step, prefix='train')\n",
        "                  step += 1\n",
        "\n",
        "              if idx % 20_000 == 0 and idx != 0:\n",
        "                self.scheduler.step()\n",
        "\n",
        "              self.optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              self.optimizer.step()\n",
        "\n",
        "          train_accuracy = correct/total\n",
        "          mlflow.log_metric(\"train accuracy__\",train_accuracy, step=epoch)\n",
        "\n",
        "          self.log_multiple_metrics(metrics_train, step=step, prefix='train')\n",
        "          self.validation(testing_loader, epoch)\n",
        "        \n",
        "\n",
        "  def validation(self, testing_loader, epoch):\n",
        "    self.model.eval()\n",
        "\n",
        "    metrics_test = {\n",
        "    \"step\":0, \"accuracy\":0, \n",
        "    \"macro avg\": {\"f1-score\":0, \"precision\":0},\n",
        "    \"neg\": {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0},\n",
        "    \"neu\": {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0},\n",
        "    \"pos\": {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0}}\n",
        "    \n",
        "    total, correct = 0,0\n",
        "    with torch.no_grad():\n",
        "        for idx, data in tqdm(enumerate(testing_loader), total=len(testing_loader)):\n",
        "            ids = data['ids'].to(self.device, dtype = torch.long)\n",
        "            mask = data['mask'].to(self.device, dtype = torch.long)\n",
        "            targets = data['targets'].to(self.device, dtype = torch.long)\n",
        "\n",
        "            outputs = self.model(ids, mask) \n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()  \n",
        "\n",
        "            self.handle_metrics_multiclass(metrics_test, predicted, targets)\n",
        "\n",
        "    mlflow.log_metric(\"test accuracy__\", correct/total, step=epoch)\n",
        "\n",
        "    self.log_multiple_metrics(metrics_test, step=epoch, prefix='test')\n",
        "\n",
        "    torch.save(self.model, f\"{self.path}{self.model.name}.pt\")\n",
        "\n",
        "    torch.save({\"optimizer\": self.optimizer.state_dict(), \n",
        "                \"scheduler\": self.scheduler.state_dict(), \"epoch\": epoch}, \n",
        "                f\"{self.path}{self.model.name}-re_train_args.pt\")"
      ],
      "metadata": {
        "id": "NWpUirUBtQJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = True\n",
        "\n",
        "if train:\n",
        "  !databricks configure --host https://community.cloud.databricks.com/\n",
        "  mlflow.set_tracking_uri('databricks')\n",
        "  mlflow.set_experiment(\"/Users/gabriele.ghisleni01@gmail.com/Bertino_Tweets\")"
      ],
      "metadata": {
        "id": "H7NXbIGBtRW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = True\n",
        "if train:\n",
        "  PARAMS = {\"epochs\":4, \"lr\":1e-04, \"batch_size\":16, \"model_name\": 'eng_distilber', 'gamma':0.96,\n",
        "            \"tokenizer_max_lenght\":25, 'trainable_layers':[], \"train-test\": (len(train_df), len(test_df))}\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=True)\n",
        "  training_loader, testing_loader = create_dataset(train_df, test_df, tokenizer, PARAMS['tokenizer_max_lenght'], PARAMS['batch_size'])\n",
        "\n",
        "  model = DistilBertTweet(name=PARAMS['model_name'])\n",
        "  loss_fn = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(params=model.parameters(), lr=PARAMS['lr'])\n",
        "  scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=PARAMS['gamma'])\n",
        "\n",
        "  # model = torch.load('./model/bert_simple.pt')\n",
        "  # optimizer.load_state_dict(torch.load('./model/bert_simple-optimizer.pt'))\n",
        "  \n",
        "  print(model, device)\n",
        "\n",
        "  for name, param in model.named_parameters():\n",
        "    if \"bert\" in name: param.requires_grad = False\n",
        "    if param.requires_grad:\n",
        "      PARAMS['trainable_layers'].append(name)\n",
        "      print(f\"Layer to train --> {name}\")\n",
        "  \n",
        "  print(PARAMS)\n",
        "  trainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer, scheduler=scheduler, epoch=PARAMS['epochs'],  path='./model/')\n",
        "  trainer.training_loop(training_loader, testing_loader, run_name=PARAMS['model_name'], **PARAMS)"
      ],
      "metadata": {
        "id": "bSL8PPyFtSRw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}