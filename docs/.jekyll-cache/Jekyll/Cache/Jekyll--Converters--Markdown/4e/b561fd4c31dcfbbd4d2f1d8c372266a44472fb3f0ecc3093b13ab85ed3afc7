I"u+<h1 id="shrinkage-methods-trees-and-forests">Shrinkage Methods, Trees and Forests</h1>
<p>Ghisleni Gabriele
20/4/2021</p>

<blockquote>
  <blockquote>
    <h2 id="exercise-1">Exercise 1</h2>

    <p>Consider the “fat” dataset provided for this Homework (tab-separated
fat.tsv). It contains percent body fat, age, weight, height and body
circumference measurements for 252 male subjects. Our goal is to
predict body fat (variable y in the dataset) from the other
explanatory variables.</p>
  </blockquote>
</blockquote>

<blockquote>
  <h3 id="1-load-the-data-and-perform-a-first-exploratory-analysis">1. Load the data and perform a first exploratory analysis</h3>
</blockquote>

<p>We start loading our data set and perform some data-exploratory
analysis. For instance we will see how the data looks like and then
check the format, the possible presence of NA’s, the descriptive
statistics and we will plot some features to understand properly our
data set. Let’s start by seeing what is inside our data.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">pander</span><span class="p">)</span><span class="w">
</span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">read.table</span><span class="p">(</span><span class="s2">"fat.tsv"</span><span class="p">,</span><span class="w"> </span><span class="n">header</span><span class="o">=</span><span class="nb">T</span><span class="p">)</span><span class="w">
</span><span class="n">pander</span><span class="p">(</span><span class="n">head</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="m">2</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center">y</th>
      <th style="text-align: center">siri</th>
      <th style="text-align: center">density</th>
      <th style="text-align: center">age</th>
      <th style="text-align: center">weight</th>
      <th style="text-align: center">height</th>
      <th style="text-align: center">neck</th>
      <th style="text-align: center">chest</th>
      <th style="text-align: center">abdomen</th>
      <th style="text-align: center">hip</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">12.6</td>
      <td style="text-align: center">12.3</td>
      <td style="text-align: center">1.071</td>
      <td style="text-align: center">23</td>
      <td style="text-align: center">154.2</td>
      <td style="text-align: center">67.75</td>
      <td style="text-align: center">36.2</td>
      <td style="text-align: center">93.1</td>
      <td style="text-align: center">85.2</td>
      <td style="text-align: center">94.5</td>
    </tr>
    <tr>
      <td style="text-align: center">6.9</td>
      <td style="text-align: center">6.1</td>
      <td style="text-align: center">1.085</td>
      <td style="text-align: center">22</td>
      <td style="text-align: center">173.2</td>
      <td style="text-align: center">72.25</td>
      <td style="text-align: center">38.5</td>
      <td style="text-align: center">93.6</td>
      <td style="text-align: center">83</td>
      <td style="text-align: center">98.7</td>
    </tr>
  </tbody>
</table>

<p>Table continues below</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">thigh</th>
      <th style="text-align: center">knee</th>
      <th style="text-align: center">ankle</th>
      <th style="text-align: center">biceps</th>
      <th style="text-align: center">forearm</th>
      <th style="text-align: center">wrist</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">59</td>
      <td style="text-align: center">37.3</td>
      <td style="text-align: center">21.9</td>
      <td style="text-align: center">32</td>
      <td style="text-align: center">27.4</td>
      <td style="text-align: center">17.1</td>
    </tr>
    <tr>
      <td style="text-align: center">58.7</td>
      <td style="text-align: center">37.3</td>
      <td style="text-align: center">23.4</td>
      <td style="text-align: center">30.5</td>
      <td style="text-align: center">28.9</td>
      <td style="text-align: center">18.2</td>
    </tr>
  </tbody>
</table>

<p>We start by checking if there are some NA’s and in case handle them:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">print</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="nf">is.na</span><span class="p">(</span><span class="n">data</span><span class="p">)]))</span><span class="w">  </span><span class="c1">#counting NA's</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] 0
</code></pre></div></div>

<p>We do not have any NA’s. We proceed looking how our date are encoded
checking the type of each variable:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">str</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## 'data.frame':    252 obs. of  16 variables:
##  $ y      : num  12.6 6.9 24.6 10.9 27.8 20.6 19 12.8 5.1 12 ...
##  $ siri   : num  12.3 6.1 25.3 10.4 28.7 20.9 19.2 12.4 4.1 11.7 ...
##  $ density: num  1.07 1.09 1.04 1.08 1.03 ...
##  $ age    : int  23 22 22 26 24 24 26 25 25 23 ...
##  $ weight : num  154 173 154 185 184 ...
##  $ height : num  67.8 72.2 66.2 72.2 71.2 ...
##  $ neck   : num  36.2 38.5 34 37.4 34.4 39 36.4 37.8 38.1 42.1 ...
##  $ chest  : num  93.1 93.6 95.8 101.8 97.3 ...
##  $ abdomen: num  85.2 83 87.9 86.4 100 94.4 90.7 88.5 82.5 88.6 ...
##  $ hip    : num  94.5 98.7 99.2 101.2 101.9 ...
##  $ thigh  : num  59 58.7 59.6 60.1 63.2 66 58.4 60 62.9 63.1 ...
##  $ knee   : num  37.3 37.3 38.9 37.3 42.2 42 38.3 39.4 38.3 41.7 ...
##  $ ankle  : num  21.9 23.4 24 22.8 24 25.6 22.9 23.2 23.8 25 ...
##  $ biceps : num  32 30.5 28.8 32.4 32.2 35.7 31.9 30.5 35.9 35.6 ...
##  $ forearm: num  27.4 28.9 25.2 29.4 27.7 30.6 27.8 29 31.1 30 ...
##  $ wrist  : num  17.1 18.2 16.6 18.2 17.7 18.8 17.7 18.8 18.2 19.2 ...
</code></pre></div></div>

<p>We have only numerical variables, included also our variable target (y
which is the body fat).</p>

<p>Now we try to understand if there are some correlations between the
variables and how they are related to the y.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">ggcorrplot</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggthemes</span><span class="p">)</span><span class="w">

</span><span class="n">cov_matrix</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="w">
</span><span class="n">ggcorrplot</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"square"</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"lower"</span><span class="p">,</span><span class="w"> </span><span class="n">title</span><span class="o">=</span><span class="s2">"Correlation plot\n"</span><span class="p">,</span><span class="w"> 
           </span><span class="n">ggtheme</span><span class="o">=</span><span class="n">theme_tufte</span><span class="p">(),</span><span class="w"> </span><span class="n">show.diag</span><span class="o">=</span><span class="nb">F</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-5-1.png" alt="" /><!-- --></p>

<p>We removed the diagonal so to remove redundant information We start
analyzing the variables that seems more correlated to our y. I arbitrary
choose the ones with corr grater than 0.6 or corr less than -0.6</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pander</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">[</span><span class="n">cov_matrix</span><span class="p">[,</span><span class="m">1</span><span class="p">]</span><span class="o">&gt;</span><span class="m">0.6</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">cov_matrix</span><span class="p">[,</span><span class="m">1</span><span class="p">]</span><span class="o">&lt;</span><span class="p">(</span><span class="m">-0.6</span><span class="p">),</span><span class="m">1</span><span class="p">],</span><span class="w"> 
       </span><span class="n">caption</span><span class="o">=</span><span class="s2">"Correlation with y"</span><span class="p">,</span><span class="w"> </span><span class="n">justify</span><span class="o">=</span><span class="s2">"center"</span><span class="w"> </span><span class="p">)</span><span class="w"> 
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center">y</th>
      <th style="text-align: center">siri</th>
      <th style="text-align: center">density</th>
      <th style="text-align: center">weight</th>
      <th style="text-align: center">chest</th>
      <th style="text-align: center">abdomen</th>
      <th style="text-align: center">hip</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.9997</td>
      <td style="text-align: center">-0.9881</td>
      <td style="text-align: center">0.6132</td>
      <td style="text-align: center">0.7029</td>
      <td style="text-align: center">0.8137</td>
      <td style="text-align: center">0.6257</td>
    </tr>
  </tbody>
</table>

<p>From this correlation plot we can see that there are few variables that
seems correlated to our variable target.</p>

<p><strong>we can also see that we have two independent variables that are very
highly correlated with the body fat, we definitely should remove one of
them (or both) because the risk to run into the multi-collinearity
problems</strong>. [I noticed that the 5. is related to this so i do not
remove them on purpose]</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">gridExtra</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggthemes</span><span class="p">)</span><span class="w">
</span><span class="n">g1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">siri</span><span class="p">),</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"royalblue"</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">0.7</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Siri"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">g2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">density</span><span class="p">),</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"firebrick3"</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">0.7</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Density"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">g3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">hip</span><span class="p">),</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"gold"</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">0.7</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Hip"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">g4</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">abdomen</span><span class="p">),</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"salmon"</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">0.7</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Abdomen"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">g5</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">weight</span><span class="p">),</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"green"</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">0.7</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Weight"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">g6</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">chest</span><span class="p">),</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"khaki3"</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">0.7</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Chest"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">grid.arrange</span><span class="p">(</span><span class="n">g1</span><span class="p">,</span><span class="n">g2</span><span class="p">,</span><span class="n">g3</span><span class="p">,</span><span class="n">g4</span><span class="p">,</span><span class="n">g5</span><span class="p">,</span><span class="n">g6</span><span class="p">,</span><span class="w"> </span><span class="n">top</span><span class="o">=</span><span class="n">textGrob</span><span class="p">(</span><span class="s2">"Y ~ Most correlated variables\n"</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-7-1.png" alt="" /><!-- --></p>

<p>As shown in the plot above we can see that we have two variabiles
perfectly correlated with the target variable: ‘siri’ and ‘density’ have
almost a perfect positive and negative correlation respectively of
0.9997, -0,9881.</p>

<p>Let’s also see the distribution of these variables:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">siri</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_histogram</span><span class="p">(</span><span class="n">fill</span><span class="o">=</span><span class="s2">"royalblue"</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="m">30</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Siri"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">g2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">density</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_histogram</span><span class="p">(</span><span class="n">fill</span><span class="o">=</span><span class="s2">"firebrick3"</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="m">30</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Density"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">g3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">hip</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_histogram</span><span class="p">(</span><span class="n">fill</span><span class="o">=</span><span class="s2">"gold"</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="m">30</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Hip"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">g4</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">abdomen</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_histogram</span><span class="p">(</span><span class="n">fill</span><span class="o">=</span><span class="s2">"salmon"</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="m">30</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Abdome"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">g5</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">weight</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_histogram</span><span class="p">(</span><span class="n">fill</span><span class="o">=</span><span class="s2">"green"</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="m">30</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Weight"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">g6</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">chest</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_histogram</span><span class="p">(</span><span class="n">fill</span><span class="o">=</span><span class="s2">"khaki3"</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="m">30</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Chest"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">

</span><span class="n">grid.arrange</span><span class="p">(</span><span class="n">g1</span><span class="p">,</span><span class="n">g2</span><span class="p">,</span><span class="n">g3</span><span class="p">,</span><span class="n">g4</span><span class="p">,</span><span class="n">g5</span><span class="p">,</span><span class="n">g6</span><span class="p">,</span><span class="w"> 
             </span><span class="n">top</span><span class="o">=</span><span class="n">textGrob</span><span class="p">(</span><span class="s2">"Histogram distribution of the most important variable\n"</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-8-1.png" alt="" /><!-- --></p>

<p>We can see that the distributions are almost standard, but seems that
there are some outliers in each of these features. So before conclude
this part we will see boxplots to understand the situation with
outliers.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">siri</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_boxplot</span><span class="p">(</span><span class="n">fill</span><span class="o">=</span><span class="s2">"royalblue"</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Siri"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">g2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">density</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_boxplot</span><span class="p">(</span><span class="n">fill</span><span class="o">=</span><span class="s2">"firebrick3"</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Density"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">g3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">hip</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_boxplot</span><span class="p">(</span><span class="n">fill</span><span class="o">=</span><span class="s2">"gold"</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Hip"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">g4</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">abdomen</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_boxplot</span><span class="p">(</span><span class="n">fill</span><span class="o">=</span><span class="s2">"salmon"</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Abdome"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">g5</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">weight</span><span class="p">))</span><span class="o">+</span><span class="w">
   </span><span class="n">geom_boxplot</span><span class="p">(</span><span class="n">fill</span><span class="o">=</span><span class="s2">"green"</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Weight"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">g6</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">chest</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_boxplot</span><span class="p">(</span><span class="n">fill</span><span class="o">=</span><span class="s2">"khaki3"</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Chest"</span><span class="p">)</span><span class="o">+</span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">

</span><span class="n">grid.arrange</span><span class="p">(</span><span class="n">g1</span><span class="p">,</span><span class="n">g2</span><span class="p">,</span><span class="n">g3</span><span class="p">,</span><span class="n">g4</span><span class="p">,</span><span class="n">g5</span><span class="p">,</span><span class="n">g6</span><span class="p">,</span><span class="w"> 
             </span><span class="n">top</span><span class="o">=</span><span class="n">textGrob</span><span class="p">(</span><span class="s2">"BoxPlot of the most importance variables\n"</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-9-1.png" alt="" /><!-- --></p>

<p>From here we can see that there are many outliers in particular in the
variable thigh, chest, abdomen and hip. Let’s also analyze our target
variable:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mu</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">round</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">y</span><span class="p">),</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">sdt</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">round</span><span class="p">(</span><span class="n">sd</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">y</span><span class="p">),</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0</span><span class="o">:</span><span class="m">50</span><span class="w">
</span><span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dnorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">mu</span><span class="p">,</span><span class="w"> </span><span class="n">sdt</span><span class="p">)</span><span class="w">
</span><span class="n">distribution_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">Y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span><span class="w">

</span><span class="n">g1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_boxplot</span><span class="p">(</span><span class="n">fill</span><span class="o">=</span><span class="s2">"darkseagreen4"</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Boxplot of y"</span><span class="p">)</span><span class="w">

</span><span class="n">g2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">..density..</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_histogram</span><span class="p">(</span><span class="n">fill</span><span class="o">=</span><span class="s2">"darkseagreen4"</span><span class="p">,</span><span class="n">colour</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="m">30</span><span class="p">)</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">distribution_y</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span><span class="w"> 
  </span><span class="n">colour</span><span class="o">=</span><span class="n">paste</span><span class="p">(</span><span class="s2">"Normal distribution ("</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="s2">","</span><span class="p">,</span><span class="n">sdt</span><span class="p">,</span><span class="s2">")"</span><span class="p">)),</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">1.5</span><span class="p">)</span><span class="o">+</span><span class="w">
  </span><span class="n">scale_color_manual</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"indianred4"</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="n">theme</span><span class="p">(</span><span class="n">legend.position</span><span class="o">=</span><span class="s2">"top"</span><span class="p">)</span><span class="o">+</span><span class="n">labs</span><span class="p">(</span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"Histogram of y"</span><span class="p">)</span><span class="w">

</span><span class="n">grid.arrange</span><span class="p">(</span><span class="n">g1</span><span class="p">,</span><span class="n">g2</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="o">=</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">top</span><span class="o">=</span><span class="n">textGrob</span><span class="p">(</span><span class="s2">"Body fat\n"</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-10-1.png" alt="" /><!-- --></p>

<p>Now that we have a rough idea of our data, we can start perform data
analysis.</p>

<blockquote>
  <h3 id="2-split-the-data-into-traintest">2. Split the data into train/test</h3>
</blockquote>

<p>We will use a 66-33 proportion in our splitting.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">r</span><span class="w"> </span><span class="o">&lt;-</span><span class="n">sample</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="w"> </span><span class="n">nrow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">*</span><span class="m">0.33</span><span class="p">)</span><span class="w">
</span><span class="n">train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">r</span><span class="p">,]</span><span class="w">
</span><span class="n">test_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="m">-1</span><span class="p">]</span><span class="w">
</span><span class="n">test_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">]</span><span class="w">
</span></code></pre></div></div>

<blockquote>
  <h3 id="3-perform-least-squares-regression-to-predict-y-from-the-other-variables">3. Perform least squares regression to predict y from the other variables.</h3>
</blockquote>

<p>We fit a simple linear regression and try to investigate how it behave.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">reg1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">~</span><span class="n">.</span><span class="p">)</span><span class="w">
</span><span class="n">pander</span><span class="p">(</span><span class="n">summary</span><span class="p">(</span><span class="n">reg1</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">Estimate</th>
      <th style="text-align: center">Std. Error</th>
      <th style="text-align: center">t value</th>
      <th style="text-align: center">Pr(&gt;|t|)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>(Intercept)</strong></td>
      <td style="text-align: center">10.03</td>
      <td style="text-align: center">4.529</td>
      <td style="text-align: center">2.213</td>
      <td style="text-align: center">0.02834</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>siri</strong></td>
      <td style="text-align: center">0.9022</td>
      <td style="text-align: center">0.009593</td>
      <td style="text-align: center">94.05</td>
      <td style="text-align: center">2.801e-137</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>density</strong></td>
      <td style="text-align: center">-9.344</td>
      <td style="text-align: center">4.053</td>
      <td style="text-align: center">-2.305</td>
      <td style="text-align: center">0.02249</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>age</strong></td>
      <td style="text-align: center">6.033e-05</td>
      <td style="text-align: center">0.00171</td>
      <td style="text-align: center">0.03527</td>
      <td style="text-align: center">0.9719</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>weight</strong></td>
      <td style="text-align: center">-0.003338</td>
      <td style="text-align: center">0.003192</td>
      <td style="text-align: center">-1.045</td>
      <td style="text-align: center">0.2975</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>height</strong></td>
      <td style="text-align: center">0.01152</td>
      <td style="text-align: center">0.009023</td>
      <td style="text-align: center">1.276</td>
      <td style="text-align: center">0.2038</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>neck</strong></td>
      <td style="text-align: center">-0.01179</td>
      <td style="text-align: center">0.01159</td>
      <td style="text-align: center">-1.017</td>
      <td style="text-align: center">0.3106</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>chest</strong></td>
      <td style="text-align: center">0.007501</td>
      <td style="text-align: center">0.005542</td>
      <td style="text-align: center">1.354</td>
      <td style="text-align: center">0.1779</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>abdomen</strong></td>
      <td style="text-align: center">0.006506</td>
      <td style="text-align: center">0.005755</td>
      <td style="text-align: center">1.131</td>
      <td style="text-align: center">0.26</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>hip</strong></td>
      <td style="text-align: center">0.002835</td>
      <td style="text-align: center">0.008441</td>
      <td style="text-align: center">0.3359</td>
      <td style="text-align: center">0.7374</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>thigh</strong></td>
      <td style="text-align: center">0.01565</td>
      <td style="text-align: center">0.007036</td>
      <td style="text-align: center">2.225</td>
      <td style="text-align: center">0.02756</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>knee</strong></td>
      <td style="text-align: center">-0.03282</td>
      <td style="text-align: center">0.01323</td>
      <td style="text-align: center">-2.48</td>
      <td style="text-align: center">0.01422</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>ankle</strong></td>
      <td style="text-align: center">0.002317</td>
      <td style="text-align: center">0.009943</td>
      <td style="text-align: center">0.233</td>
      <td style="text-align: center">0.816</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>biceps</strong></td>
      <td style="text-align: center">-0.01236</td>
      <td style="text-align: center">0.008852</td>
      <td style="text-align: center">-1.396</td>
      <td style="text-align: center">0.1648</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>forearm</strong></td>
      <td style="text-align: center">0.01754</td>
      <td style="text-align: center">0.01008</td>
      <td style="text-align: center">1.739</td>
      <td style="text-align: center">0.08397</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>wrist</strong></td>
      <td style="text-align: center">0.01466</td>
      <td style="text-align: center">0.02741</td>
      <td style="text-align: center">0.5349</td>
      <td style="text-align: center">0.5935</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Observations</th>
      <th style="text-align: center">Residual Std. Error</th>
      <th style="text-align: center"><em>R</em><sup>2</sup></th>
      <th style="text-align: center">Adjusted <em>R</em><sup>2</sup></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">169</td>
      <td style="text-align: center">0.177</td>
      <td style="text-align: center">0.9995</td>
      <td style="text-align: center">0.9995</td>
    </tr>
  </tbody>
</table>

<p>Fitting linear model: y ~ .</p>

<p>Using a standard linear regression we can see that the variables having
a lower p-value (which means the probability to see those data if the
feature is not significant aka =0) are: ‘density’, ‘siri’ and ‘thigh’,
‘knee’ and last ‘forearm’ with p-value around 8%. the R^2 is very low
and also the residual standard error is very low.</p>

<p>Let’s see the MSE on test error with this model:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_prediction</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">reg1</span><span class="p">,</span><span class="w"> </span><span class="n">test_x</span><span class="p">)</span><span class="w">
</span><span class="n">MSE_N</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">((</span><span class="n">test_y</span><span class="o">-</span><span class="n">test_prediction</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"The mean square error on the test set is: "</span><span class="p">,</span><span class="n">MSE_N</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "The mean square error on the test set is:  0.0343231476007284"
</code></pre></div></div>

<p>The mean square error is very low and the R^2 is very high. Seems that
we are performing a perfect regression. <em>Is possible that something
strange is happening</em>.</p>

<blockquote>
  <h3 id="4-apply-ridge-regression-and-the-lasso-to-the-same-data">4. Apply ridge regression and the lasso to the same data.</h3>
</blockquote>

<ul>
  <li>Plot the coefficients as a function of lambda</li>
  <li>Plot the cross-validation MSE as a function of lambda and find the
optimal lambda</li>
  <li>Compute the test MSE of the optimal model</li>
  <li>Examine the coefficients of the optimal mode</li>
</ul>

<blockquote>
  <h3 id="1-ridge-regression">1) Ridge Regression</h3>
</blockquote>

<p>Since we have many coefficients we can perform shrinkage regression as
ridge or lasso. Those models basically change the way that the
regression line is found adding a constrained in the formula (as in
Lagrange optimization) that limits the impact of predictors.</p>

<p>in Ridge regression the formula is:
\(\\mathit{\\widehat{\\beta}= arg\\ min \\ RSS + \\lambda \\sum\_{j=1}^{p} \\beta\_{j}^{2}}\)</p>

<p>The result will be a model having more bias but at the same time less
variance. <em>We also perform a standardization of the variables since this
affect the ridge and lasso regression!</em></p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># We must transform the data frame into a matrix (requirement of the function)</span><span class="w">
</span><span class="n">train_mat_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="n">model.matrix</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">r</span><span class="p">,])[,</span><span class="w"> </span><span class="m">-1</span><span class="p">])</span><span class="w">
</span><span class="n">test_mat_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="n">model.matrix</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">r</span><span class="p">,])[,</span><span class="w"> </span><span class="m">-1</span><span class="p">])</span><span class="w">
</span><span class="n">train_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">r</span><span class="p">,</span><span class="m">1</span><span class="p">]</span><span class="w">
</span><span class="n">test_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">r</span><span class="p">,</span><span class="m">1</span><span class="p">]</span><span class="w">
</span><span class="n">paste</span><span class="p">(</span><span class="s2">"Dimension original df: "</span><span class="p">,</span><span class="w">
             </span><span class="nf">dim</span><span class="p">(</span><span class="n">train</span><span class="p">)[</span><span class="m">1</span><span class="p">],</span><span class="w">
             </span><span class="nf">dim</span><span class="p">(</span><span class="n">train</span><span class="p">)[</span><span class="m">2</span><span class="p">],</span><span class="w">
             </span><span class="s2">"Dimension of the new matrix (we left out y): "</span><span class="p">,</span><span class="w">
             </span><span class="nf">dim</span><span class="p">(</span><span class="n">train_mat_x</span><span class="p">)[</span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">train_mat_x</span><span class="p">)[</span><span class="m">2</span><span class="p">])</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "Dimension original df:  169 16 Dimension of the new matrix (we left out y):  169 15"
</code></pre></div></div>

<p>We start performing a ridge regression, we must specify the parameter
alpha which could be used in two different way: if <em>alpha=0 means ridge
regression</em> while alpha=1 means lasso regression. Also important is the
parameter lambda which represent the weight of the penalization of the
betas (as shown in the formula), by default R try to guess a vector,
instead if we explicit pass an array R will use those. We start perform
a normal ridge regression, see the results and then optimized the result
obtained using cross validation.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">glmnet</span><span class="p">)</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">lambdas</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">10</span><span class="o">^</span><span class="n">seq</span><span class="p">(</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="m">-2</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="o">=</span><span class="m">100</span><span class="p">)</span><span class="w">
</span><span class="n">ridge_regression</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glmnet</span><span class="p">(</span><span class="n">train_mat_x</span><span class="p">,</span><span class="w"> </span><span class="n">train_y</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="o">=</span><span class="n">lambdas</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge_regression</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-15-1.png" alt="" /><!-- --> From this plot is
clear that there are two variables that impact a lot the performance of
the model, most probably the ones that were highly correlated: ‘siri’
and ‘density’.</p>

<p>Now we perform the same operation evaluating the performance in term of
MSE with cross-validation.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ridge_regression</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.glmnet</span><span class="p">(</span><span class="n">train_mat_x</span><span class="p">,</span><span class="w"> </span><span class="n">train_y</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="o">=</span><span class="n">lambdas</span><span class="p">)</span><span class="w">
</span><span class="n">coeff_ridge</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">ridge_regression</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"coefficients"</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">=</span><span class="n">lambdas</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>We also plot the MSE ~ lambdas (and on the top the number of features
used into the model).</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">ridge_regression</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-17-1.png" alt="" /><!-- --></p>

<p>From the plot above we can see that the optimal lambda is very low, also
the confidence interval in small. we also notice as before that the MSE
is very low.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># we can also find the optimal lambda as:</span><span class="w">
</span><span class="n">opti_lamda_ridge_n</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ridge_regression</span><span class="o">$</span><span class="n">lambda.min</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"The optimal of ridge regression lambda is: "</span><span class="p">,</span><span class="n">opti_lamda_ridge_n</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "The optimal of ridge regression lambda is:  0.01"
</code></pre></div></div>

<ul>
  <li>Compute the test MSE of the optimal model</li>
</ul>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">ridge_prediction</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">ridge_regression</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">=</span><span class="n">opti_lamda_ridge_n</span><span class="p">,</span><span class="n">test_mat_x</span><span class="p">)</span><span class="w">
</span><span class="n">MSE_ridge_N</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">((</span><span class="n">ridge_prediction</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">test_y</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"MSE Test Ridge (optimal lambda):"</span><span class="p">,</span><span class="w"> </span><span class="n">MSE_ridge_N</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "MSE Test Ridge (optimal lambda): 0.501108803453915"
</code></pre></div></div>

<p>The MSE is slightly higher than a normal linear regression. This could
means that the variance inside the data is very low and biased model
performs worst than an unbiased model.</p>

<ul>
  <li>Examine the coefficients of the optimal model</li>
</ul>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">ridge_coeff_best_lambda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">ridge_regression</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"coefficients"</span><span class="p">,</span><span class="w">
                                   </span><span class="n">s</span><span class="o">=</span><span class="n">ridge_regression</span><span class="o">$</span><span class="n">lambda.min</span><span class="p">)</span><span class="w">

</span><span class="n">tmp</span><span class="o">&lt;-</span><span class="n">data.frame</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">ridge_coeff_best_lambda</span><span class="o">@</span><span class="n">Dimnames</span><span class="p">[</span><span class="m">1</span><span class="p">],</span><span class="w"> 
                </span><span class="n">value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ridge_coeff_best_lambda</span><span class="o">@</span><span class="n">x</span><span class="p">)</span><span class="w">
</span><span class="n">colnames</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"Names"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Values"</span><span class="p">)</span><span class="w">
</span><span class="n">pander</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Names</th>
      <th style="text-align: center">Values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">(Intercept)</td>
      <td style="text-align: center">18.71</td>
    </tr>
    <tr>
      <td style="text-align: center">siri</td>
      <td style="text-align: center">7.197</td>
    </tr>
    <tr>
      <td style="text-align: center">density</td>
      <td style="text-align: center">-0.5136</td>
    </tr>
    <tr>
      <td style="text-align: center">age</td>
      <td style="text-align: center">0.01446</td>
    </tr>
    <tr>
      <td style="text-align: center">weight</td>
      <td style="text-align: center">-0.112</td>
    </tr>
    <tr>
      <td style="text-align: center">height</td>
      <td style="text-align: center">0.0388</td>
    </tr>
    <tr>
      <td style="text-align: center">neck</td>
      <td style="text-align: center">-0.029</td>
    </tr>
    <tr>
      <td style="text-align: center">chest</td>
      <td style="text-align: center">0.08883</td>
    </tr>
    <tr>
      <td style="text-align: center">abdomen</td>
      <td style="text-align: center">0.08268</td>
    </tr>
    <tr>
      <td style="text-align: center">hip</td>
      <td style="text-align: center">0.04257</td>
    </tr>
    <tr>
      <td style="text-align: center">thigh</td>
      <td style="text-align: center">0.08457</td>
    </tr>
    <tr>
      <td style="text-align: center">knee</td>
      <td style="text-align: center">-0.0942</td>
    </tr>
    <tr>
      <td style="text-align: center">ankle</td>
      <td style="text-align: center">-0.001544</td>
    </tr>
    <tr>
      <td style="text-align: center">biceps</td>
      <td style="text-align: center">-0.04753</td>
    </tr>
    <tr>
      <td style="text-align: center">forearm</td>
      <td style="text-align: center">0.04475</td>
    </tr>
    <tr>
      <td style="text-align: center">wrist</td>
      <td style="text-align: center">0.01303</td>
    </tr>
  </tbody>
</table>

<p>We can see that none of them is exactly zero, because we are performing
a Ridge regression which brings them near to zero but not exactly. The
most important variable for this ridge regression are ‘siri’ with 7
(very high) and ‘density’ with -0.5 Now that we do the same procedure
with lasso we expect to see some of them equal to zero.</p>

<blockquote>
  <h3 id="2-lasso-regression">2) Lasso Regression</h3>
</blockquote>

<p>The idea is the same as the Ridge regression, a shrinkage regression
that results in a model with high bias but low variance. in this case
the formula is slightly different and it bring down to zero the
coefficients that are useless.</p>

<p>Lasso Formula:
\(\\mathit{\\widehat{\\beta}= arg\\ min\\ RSS + \\lambda \\sum\_{j=1}^{p} \\left \\\|\\beta\_{j}\\right \\\|}\)</p>

<p>As we did before we prepare the data:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_mat_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="n">model.matrix</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">r</span><span class="p">,])[,</span><span class="w"> </span><span class="m">-1</span><span class="p">])</span><span class="w">
</span><span class="n">test_mat_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="n">model.matrix</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">r</span><span class="p">,])[,</span><span class="w"> </span><span class="m">-1</span><span class="p">])</span><span class="w">
</span><span class="n">train_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">r</span><span class="p">,</span><span class="m">1</span><span class="p">]</span><span class="w">
</span><span class="n">test_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">r</span><span class="p">,</span><span class="m">1</span><span class="p">]</span><span class="w">
</span></code></pre></div></div>

<p>We change the parameter alpha from 0 to 1, as we said before <strong>alpha = 1
means Lasso</strong>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">lambdas</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">10</span><span class="o">^</span><span class="n">seq</span><span class="p">(</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="m">-2</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="o">=</span><span class="m">100</span><span class="p">)</span><span class="w">
</span><span class="n">lasso_regression</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glmnet</span><span class="p">(</span><span class="n">train_mat_x</span><span class="p">,</span><span class="w"> </span><span class="n">train_y</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="o">=</span><span class="n">lambdas</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">lasso_regression</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-22-1.png" alt="" /><!-- --></p>

<p>There are almost all zero except for once, which is very bad. basically
the model use just one variable to make the prediction, which most
probably is going to be ‘siri’.</p>

<p>As before first we plot the curvature of the features according to the
lambdas and then we find the best option with cross validation.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lasso_regression</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.glmnet</span><span class="p">(</span><span class="n">train_mat_x</span><span class="p">,</span><span class="w"> </span><span class="n">train_y</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="o">=</span><span class="n">lambdas</span><span class="p">)</span><span class="w"> 
</span><span class="n">coeff_lasso</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">lasso_regression</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"coefficients"</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">=</span><span class="n">lambdas</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<ul>
  <li>Plot the cross-validation MSE as a function of lambda and find the
optimal</li>
</ul>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">lasso_regression</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-24-1.png" alt="" /><!-- --> Different from
before in top of the plot we have the number of variables different from
zero used in the lasso regression. When lambda increase, in this case
when lambda is greater than exp(4) all the variables are dropped out
from the models. From this plot R is telling us that the best choice of
lambda bring to use just two variable (probably the most correlated),
let’s see better which is the optimal lambda and what are the features
that are different from zero.</p>

<ul>
  <li>Optimal Lambda:</li>
</ul>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optima_lambda_n</span><span class="o">&lt;-</span><span class="n">lasso_regression</span><span class="o">$</span><span class="n">lambda.min</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"The optimal lambda for lasso regression is: "</span><span class="p">,</span><span class="n">optima_lambda_n</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "The optimal lambda for lasso regression is:  0.0305385550883341"
</code></pre></div></div>

<ul>
  <li>MSE in test set:</li>
</ul>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">lasso_prediction</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">lasso_regression</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">=</span><span class="n">optima_lambda_n</span><span class="p">,</span><span class="w"> </span><span class="n">test_mat_x</span><span class="p">)</span><span class="w">
</span><span class="n">MSE_Lasso_n</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">((</span><span class="n">lasso_prediction</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">test_y</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"MSE Test Lasso (optimal lambda) ="</span><span class="p">,</span><span class="n">MSE_Lasso_n</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "MSE Test Lasso (optimal lambda) = 0.489189788278689"
</code></pre></div></div>

<p>The MSE is slightly higher than a normal regression but very similar to
the ridge regression MSE.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">lasso_coeff_best_lambda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">lasso_regression</span><span class="p">,</span><span class="w"> 
                                   </span><span class="n">type</span><span class="o">=</span><span class="s2">"coefficients"</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">=</span><span class="n">lasso_regression</span><span class="o">$</span><span class="n">lambda.min</span><span class="p">)</span><span class="w">
</span><span class="c1">#create the data frame just to represent purpose. to check remove comment the line below.</span><span class="w">
</span><span class="c1">#lasso_coeff_best_lambda #the . stand for zero</span><span class="w">
</span><span class="n">tmp</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">Names</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"Intercept"</span><span class="p">,</span><span class="s2">"siri"</span><span class="p">,</span><span class="s2">"density"</span><span class="p">,</span><span class="s2">"abdomen"</span><span class="p">),</span><span class="w"> 
                 </span><span class="n">values</span><span class="o">=</span><span class="n">lasso_coeff_best_lambda</span><span class="o">@</span><span class="n">x</span><span class="p">)</span><span class="w">
</span><span class="n">pander</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Names</th>
      <th style="text-align: center">values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Intercept</td>
      <td style="text-align: center">18.71</td>
    </tr>
    <tr>
      <td style="text-align: center">siri</td>
      <td style="text-align: center">7.662</td>
    </tr>
    <tr>
      <td style="text-align: center">density</td>
      <td style="text-align: center">-0.09804</td>
    </tr>
    <tr>
      <td style="text-align: center">abdomen</td>
      <td style="text-align: center">0.0001307</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <h4 id="5-critically-evaluate-the-results-you-obtained-if-they-look-suspicious-think-about-a-possible-cause">5. Critically evaluate the results you obtained. If they look suspicious, think about a possible cause.</h4>

  <p>For example, examine the coefficients of the least square regression
model (estimate and sign), together with the R2 value; compute the
pairwise correlations between the variables, . . . Think of a
modification of the analysis in light of your findings and repeat
steps 1-4 of your new analysis. Comment on the new results.</p>
</blockquote>

<p>The results look suspicious, there are a perfect correlation between the
y and two other variable: 0.9777 with ‘siry’ and -0.9881 with ‘density’.
the R^2 is also perfect.</p>

<p>After have done a quick research i found that ‘siry’ and ‘density’ are
basically the same as the body fat so having them in the predictors mean
that the model will use only those.</p>

<p>The result is correct but since having those implies in a certain way
already know the body fat we are not increasing our knowledge on the
problem (since we are using the equivalent of body fact to predict body
fact) so i decided to remove them and see what are the predictors inside
our data set that can be used to predict it without already have the
answer encoded in a different variable..</p>

<p>So now we perform the same operations this time removing ‘siri’ and
‘density’. From now on we will call <em>adjusted</em> all the models without
those two variables.</p>

<blockquote>
  <h3 id="1-normal-regression-adjusted">1) Normal regression adjusted</h3>
</blockquote>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">r</span><span class="w"> </span><span class="o">&lt;-</span><span class="n">sample</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="w"> </span><span class="n">nrow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">*</span><span class="m">0.33</span><span class="p">)</span><span class="w">
</span><span class="n">train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="m">-2</span><span class="o">:</span><span class="m">-3</span><span class="p">]</span><span class="w">
</span><span class="n">test_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="m">-1</span><span class="o">:</span><span class="m">-3</span><span class="p">]</span><span class="w">
</span><span class="n">test_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">]</span><span class="w">
</span><span class="n">reg1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">~</span><span class="n">.</span><span class="p">)</span><span class="w">
</span><span class="n">pander</span><span class="p">(</span><span class="n">summary</span><span class="p">(</span><span class="n">reg1</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">Estimate</th>
      <th style="text-align: center">Std. Error</th>
      <th style="text-align: center">t value</th>
      <th style="text-align: center">Pr(&gt;|t|)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>(Intercept)</strong></td>
      <td style="text-align: center">-37.06</td>
      <td style="text-align: center">26.71</td>
      <td style="text-align: center">-1.387</td>
      <td style="text-align: center">0.1673</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>age</strong></td>
      <td style="text-align: center">0.07695</td>
      <td style="text-align: center">0.03841</td>
      <td style="text-align: center">2.003</td>
      <td style="text-align: center">0.0469</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>weight</strong></td>
      <td style="text-align: center">-0.1224</td>
      <td style="text-align: center">0.07222</td>
      <td style="text-align: center">-1.695</td>
      <td style="text-align: center">0.09213</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>height</strong></td>
      <td style="text-align: center">0.09176</td>
      <td style="text-align: center">0.2054</td>
      <td style="text-align: center">0.4466</td>
      <td style="text-align: center">0.6558</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>neck</strong></td>
      <td style="text-align: center">-0.3141</td>
      <td style="text-align: center">0.2633</td>
      <td style="text-align: center">-1.193</td>
      <td style="text-align: center">0.2347</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>chest</strong></td>
      <td style="text-align: center">0.02105</td>
      <td style="text-align: center">0.1255</td>
      <td style="text-align: center">0.1678</td>
      <td style="text-align: center">0.867</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>abdomen</strong></td>
      <td style="text-align: center">0.8895</td>
      <td style="text-align: center">0.1086</td>
      <td style="text-align: center">8.194</td>
      <td style="text-align: center">8.868e-14</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>hip</strong></td>
      <td style="text-align: center">-0.08595</td>
      <td style="text-align: center">0.1918</td>
      <td style="text-align: center">-0.4482</td>
      <td style="text-align: center">0.6546</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>thigh</strong></td>
      <td style="text-align: center">0.3274</td>
      <td style="text-align: center">0.1584</td>
      <td style="text-align: center">2.067</td>
      <td style="text-align: center">0.04042</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>knee</strong></td>
      <td style="text-align: center">-0.4099</td>
      <td style="text-align: center">0.3001</td>
      <td style="text-align: center">-1.366</td>
      <td style="text-align: center">0.174</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>ankle</strong></td>
      <td style="text-align: center">0.2228</td>
      <td style="text-align: center">0.225</td>
      <td style="text-align: center">0.99</td>
      <td style="text-align: center">0.3237</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>biceps</strong></td>
      <td style="text-align: center">0.07718</td>
      <td style="text-align: center">0.201</td>
      <td style="text-align: center">0.3839</td>
      <td style="text-align: center">0.7015</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>forearm</strong></td>
      <td style="text-align: center">0.6479</td>
      <td style="text-align: center">0.2245</td>
      <td style="text-align: center">2.886</td>
      <td style="text-align: center">0.004464</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>wrist</strong></td>
      <td style="text-align: center">-1.432</td>
      <td style="text-align: center">0.6137</td>
      <td style="text-align: center">-2.333</td>
      <td style="text-align: center">0.02094</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Observations</th>
      <th style="text-align: center">Residual Std. Error</th>
      <th style="text-align: center"><em>R</em><sup>2</sup></th>
      <th style="text-align: center">Adjusted <em>R</em><sup>2</sup></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">169</td>
      <td style="text-align: center">4.041</td>
      <td style="text-align: center">0.7516</td>
      <td style="text-align: center">0.7307</td>
    </tr>
  </tbody>
</table>

<p>Fitting linear model: y ~ .</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_prediction</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">reg1</span><span class="p">,</span><span class="w"> </span><span class="n">test_x</span><span class="p">)</span><span class="w">
</span><span class="n">MSE</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">((</span><span class="n">test_y</span><span class="o">-</span><span class="n">test_prediction</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"The mean square error on the test set is: "</span><span class="p">,</span><span class="n">MSE</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "The mean square error on the test set is:  17.0771332847323"
</code></pre></div></div>

<p>We can directly see that the results will be very different. The
significant variables are now age, abdomen, thigh, forearm, wrist and
weight with p-value at 9%. The R^2 is not perfect anymore, and the
residual standard error is much higher compared to before which was
almost null.</p>

<blockquote>
  <h3 id="2-ridge-adjusted">2) Ridge adjusted</h3>
</blockquote>

<p>Since we already explain what happen before now we will only represent
the results consider the reduced data set comparing the results with
what we obtained before.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="c1"># reprocess data</span><span class="w">
</span><span class="n">train_mat_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="n">model.matrix</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">r</span><span class="p">,])[,</span><span class="w"> </span><span class="m">-3</span><span class="o">:</span><span class="m">-1</span><span class="p">])</span><span class="w">
</span><span class="n">test_mat_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="n">model.matrix</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">r</span><span class="p">,])[,</span><span class="w"> </span><span class="m">-3</span><span class="o">:</span><span class="m">-1</span><span class="p">])</span><span class="w">
</span><span class="n">train_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">r</span><span class="p">,</span><span class="m">1</span><span class="p">]</span><span class="w">
</span><span class="n">test_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">r</span><span class="p">,</span><span class="m">1</span><span class="p">]</span><span class="w">
</span><span class="c1">#create range of lambda</span><span class="w">
</span><span class="n">lambdas</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">10</span><span class="o">^</span><span class="n">seq</span><span class="p">(</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="m">-2</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="o">=</span><span class="m">100</span><span class="p">)</span><span class="w">
</span><span class="n">ridge_regression</span><span class="w"> </span><span class="o">&lt;-</span><span class="n">glmnet</span><span class="p">(</span><span class="n">train_mat_x</span><span class="p">,</span><span class="w"> </span><span class="n">train_y</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="o">=</span><span class="n">lambdas</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge_regression</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-29-1.png" alt="" /><!-- --></p>

<p>Now the variables that are not pushed down to almost zero are more and,
expect for one, have a similar weight.</p>

<p>Fit the best ridge regression with cross-validation:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#fit a ridge_regression and plot it</span><span class="w">
</span><span class="n">ridge_regression</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.glmnet</span><span class="p">(</span><span class="n">train_mat_x</span><span class="p">,</span><span class="w"> </span><span class="n">train_y</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="o">=</span><span class="n">lambdas</span><span class="p">)</span><span class="w"> 
</span><span class="n">opti_lambda_ridge</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ridge_regression</span><span class="o">$</span><span class="n">lambda.min</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge_regression</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-30-1.png" alt="" /><!-- --></p>

<p>We can see now that the results are quiet different than before, the
confidence interval is larger. let’s go on and see others detail of this
regression as the optimal lambda, the MSE and the more important
features.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"The optimal of ridge regression lambda is: "</span><span class="p">,</span><span class="n">opti_lambda_ridge</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "The optimal of ridge regression lambda is:  0.01"
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">ridge_prediction</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">ridge_regression</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">=</span><span class="n">opti_lambda_ridge</span><span class="p">,</span><span class="w"> </span><span class="n">test_mat_x</span><span class="p">)</span><span class="w">
</span><span class="n">MSE_ridge</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">((</span><span class="n">ridge_prediction</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">test_y</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"MSE Test Ridge (optimal lambda):"</span><span class="p">,</span><span class="n">MSE_ridge</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "MSE Test Ridge (optimal lambda): 16.3449890155978"
</code></pre></div></div>

<p>Now we can see that the ridge regression perform a few better than a
normal linear regression. In this case having increased the bias in the
training results in a better test performance.</p>

<p>While the coefficients are:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">ridge_coeff_best_lambda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">ridge_regression</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"coefficients"</span><span class="p">,</span><span class="w">
                                   </span><span class="n">s</span><span class="o">=</span><span class="n">opti_lambda_ridge</span><span class="p">)</span><span class="w">

</span><span class="n">tmp</span><span class="o">&lt;-</span><span class="n">data.frame</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">ridge_coeff_best_lambda</span><span class="o">@</span><span class="n">Dimnames</span><span class="p">[</span><span class="m">1</span><span class="p">],</span><span class="w"> 
                </span><span class="n">value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ridge_coeff_best_lambda</span><span class="o">@</span><span class="n">x</span><span class="p">)</span><span class="w">
</span><span class="n">colnames</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"Names"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Values"</span><span class="p">)</span><span class="w">
</span><span class="n">pander</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Names</th>
      <th style="text-align: center">Values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">(Intercept)</td>
      <td style="text-align: center">18.71</td>
    </tr>
    <tr>
      <td style="text-align: center">age</td>
      <td style="text-align: center">1.026</td>
    </tr>
    <tr>
      <td style="text-align: center">weight</td>
      <td style="text-align: center">-3.328</td>
    </tr>
    <tr>
      <td style="text-align: center">height</td>
      <td style="text-align: center">0.1834</td>
    </tr>
    <tr>
      <td style="text-align: center">neck</td>
      <td style="text-align: center">-0.7909</td>
    </tr>
    <tr>
      <td style="text-align: center">chest</td>
      <td style="text-align: center">0.1799</td>
    </tr>
    <tr>
      <td style="text-align: center">abdomen</td>
      <td style="text-align: center">9.362</td>
    </tr>
    <tr>
      <td style="text-align: center">hip</td>
      <td style="text-align: center">-0.6348</td>
    </tr>
    <tr>
      <td style="text-align: center">thigh</td>
      <td style="text-align: center">1.753</td>
    </tr>
    <tr>
      <td style="text-align: center">knee</td>
      <td style="text-align: center">-1.021</td>
    </tr>
    <tr>
      <td style="text-align: center">ankle</td>
      <td style="text-align: center">0.3834</td>
    </tr>
    <tr>
      <td style="text-align: center">biceps</td>
      <td style="text-align: center">0.2129</td>
    </tr>
    <tr>
      <td style="text-align: center">forearm</td>
      <td style="text-align: center">1.313</td>
    </tr>
    <tr>
      <td style="text-align: center">wrist</td>
      <td style="text-align: center">-1.328</td>
    </tr>
  </tbody>
</table>

<p>Now that we removed the problems of multi-collinearity and standardized
our data the results seems more reliable in fact there are more
variables that contribute to the prediction.</p>

<blockquote>
  <h3 id="3-lasso-adjusted">3) Lasso adjusted</h3>
</blockquote>

<p>Since we already explain what happen before now we will only represent
the results consider the reduced data set comparing the results with
what we obtained before.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="c1"># preprocess data</span><span class="w">
</span><span class="n">train_mat_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="n">model.matrix</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">r</span><span class="p">,])[,</span><span class="w"> </span><span class="m">-3</span><span class="o">:</span><span class="m">-1</span><span class="p">])</span><span class="w">
</span><span class="n">test_mat_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="n">model.matrix</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">r</span><span class="p">,])[,</span><span class="w"> </span><span class="m">-3</span><span class="o">:</span><span class="m">-1</span><span class="p">])</span><span class="w">
</span><span class="n">train_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">r</span><span class="p">,</span><span class="m">1</span><span class="p">]</span><span class="w">
</span><span class="n">test_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">r</span><span class="p">,</span><span class="m">1</span><span class="p">]</span><span class="w">
</span><span class="c1">#create range of lambda</span><span class="w">
</span><span class="n">lambdas</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">10</span><span class="o">^</span><span class="n">seq</span><span class="p">(</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="m">-2</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="o">=</span><span class="m">100</span><span class="p">)</span><span class="w">
</span><span class="n">lasso_regression</span><span class="w"> </span><span class="o">&lt;-</span><span class="n">glmnet</span><span class="p">(</span><span class="n">train_mat_x</span><span class="p">,</span><span class="w"> </span><span class="n">train_y</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="o">=</span><span class="n">lambdas</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">lasso_regression</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-34-1.png" alt="" /><!-- --> Seems that there
is again one variable which is stronger than the others but overall is
fine because the others are not all zero.</p>

<p>Find the best lasso regression with cross-validation:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#fit a ridge_regression and plot it</span><span class="w">
</span><span class="n">lasso_regression</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.glmnet</span><span class="p">(</span><span class="n">train_mat_x</span><span class="p">,</span><span class="w"> </span><span class="n">train_y</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="o">=</span><span class="n">lambdas</span><span class="p">)</span><span class="w"> 
</span><span class="n">opti_lambda_lasso</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lasso_regression</span><span class="o">$</span><span class="n">lambda.min</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">lasso_regression</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-35-1.png" alt="" /><!-- --></p>

<p>We can see now that the results are quiet different than before, the
number of variables suggested to the optimal lasso regression are
between 11 and 5. let’s go on and see others detail of this regression
as the optimal lambda, the MSE and the more important features.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"The optimal of Lasso regression lambda is: "</span><span class="p">,</span><span class="n">opti_lambda_lasso</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "The optimal of Lasso regression lambda is:  0.0403701725859655"
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lasso_prediction</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">lasso_regression</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">=</span><span class="n">opti_lambda_lasso</span><span class="p">,</span><span class="w"> </span><span class="n">test_mat_x</span><span class="p">)</span><span class="w">
</span><span class="n">MSE_lambda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">((</span><span class="n">lasso_prediction</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">test_y</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"MSE Test Lasso (optimal lambda):"</span><span class="p">,</span><span class="n">MSE_lambda</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "MSE Test Lasso (optimal lambda): 15.9684828449859"
</code></pre></div></div>

<p>Also in this case the performance is better than the normal linear
regression.</p>

<p>We can also control which variables are not reduced to zero:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">lasso_coeff_best_lambda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">lasso_regression</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"coefficients"</span><span class="p">,</span><span class="w">
                                   </span><span class="n">s</span><span class="o">=</span><span class="n">opti_lambda_lasso</span><span class="p">)</span><span class="w">



</span><span class="c1">#create the dataframe just to represent purpose. #to check decomment the line below. </span><span class="w">
</span><span class="c1">#lasso_coeff_best_lambda #the . stand for zero!</span><span class="w">
</span><span class="n">pander</span><span class="p">(</span><span class="n">data.frame</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"intercept"</span><span class="p">,</span><span class="n">colnames</span><span class="p">(</span><span class="n">train_mat_x</span><span class="p">)[</span><span class="m">-6</span><span class="p">]),</span><span class="w"> 
                  </span><span class="n">values</span><span class="o">=</span><span class="n">lasso_coeff_best_lambda</span><span class="o">@</span><span class="n">x</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center">name</th>
      <th style="text-align: center">values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">intercept</td>
      <td style="text-align: center">18.71</td>
    </tr>
    <tr>
      <td style="text-align: center">age</td>
      <td style="text-align: center">0.955</td>
    </tr>
    <tr>
      <td style="text-align: center">weight</td>
      <td style="text-align: center">-2.336</td>
    </tr>
    <tr>
      <td style="text-align: center">height</td>
      <td style="text-align: center">-0.07042</td>
    </tr>
    <tr>
      <td style="text-align: center">neck</td>
      <td style="text-align: center">-0.6513</td>
    </tr>
    <tr>
      <td style="text-align: center">chest</td>
      <td style="text-align: center">8.817</td>
    </tr>
    <tr>
      <td style="text-align: center">hip</td>
      <td style="text-align: center">-0.4268</td>
    </tr>
    <tr>
      <td style="text-align: center">thigh</td>
      <td style="text-align: center">1.247</td>
    </tr>
    <tr>
      <td style="text-align: center">knee</td>
      <td style="text-align: center">-0.8114</td>
    </tr>
    <tr>
      <td style="text-align: center">ankle</td>
      <td style="text-align: center">0.2113</td>
    </tr>
    <tr>
      <td style="text-align: center">biceps</td>
      <td style="text-align: center">0.04338</td>
    </tr>
    <tr>
      <td style="text-align: center">forearm</td>
      <td style="text-align: center">1.235</td>
    </tr>
    <tr>
      <td style="text-align: center">wrist</td>
      <td style="text-align: center">-1.286</td>
    </tr>
  </tbody>
</table>

<p>Now that we have all the elements we can compare them all and try to
draw some conclusion about this analysis.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df1</span><span class="w"> </span><span class="o">&lt;-</span><span class="p">(</span><span class="n">data.frame</span><span class="p">(</span><span class="w">
  </span><span class="n">Regression</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"Normal regression adj"</span><span class="p">,</span><span class="s2">"Lasso adj"</span><span class="p">,</span><span class="s2">"Ridge adj"</span><span class="p">),</span><span class="w">
  </span><span class="n">MSE</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="n">MSE</span><span class="p">,</span><span class="n">MSE_lambda</span><span class="p">,</span><span class="w"> </span><span class="n">MSE_ridge</span><span class="p">),</span><span class="w">
  </span><span class="n">optimal_lambda</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="n">opti_lambda_lasso</span><span class="p">,</span><span class="n">opti_lambda_ridge</span><span class="p">),</span><span class="w">    
  </span><span class="n">Removed_variability</span><span class="o">=</span><span class="p">(</span><span class="s2">"sini, density"</span><span class="p">)))</span><span class="w">
</span><span class="n">df2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="w">
  </span><span class="n">Regression</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"Normal regression"</span><span class="p">,</span><span class="s2">"Lasso"</span><span class="p">,</span><span class="s2">"Ridge"</span><span class="p">),</span><span class="w">
  </span><span class="n">MSE</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="n">MSE_N</span><span class="p">,</span><span class="n">MSE_Lasso_n</span><span class="p">,</span><span class="w"> </span><span class="n">MSE_ridge_N</span><span class="p">),</span><span class="w">
  </span><span class="n">optimal_lambda</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="n">optima_lambda_n</span><span class="p">,</span><span class="n">opti_lamda_ridge_n</span><span class="p">),</span><span class="w">    
  </span><span class="n">Removed_variability</span><span class="o">=</span><span class="p">(</span><span class="s2">"None"</span><span class="p">))</span><span class="w">
</span><span class="n">df_tot</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rbind</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span><span class="n">df2</span><span class="p">)</span><span class="w">
</span><span class="n">df_tot</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">df_tot</span><span class="p">[</span><span class="n">order</span><span class="p">(</span><span class="n">df_tot</span><span class="o">$</span><span class="n">Regression</span><span class="p">),]</span><span class="w">
</span><span class="n">rownames</span><span class="p">(</span><span class="n">df_tot</span><span class="p">)</span><span class="o">&lt;-</span><span class="kc">NULL</span><span class="w">
</span><span class="n">pander</span><span class="p">(</span><span class="n">df_tot</span><span class="p">,</span><span class="w"> </span><span class="n">caption</span><span class="o">=</span><span class="s2">"For adj means without sini and density"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Regression</th>
      <th style="text-align: center">MSE</th>
      <th style="text-align: center">optimal_lambda</th>
      <th style="text-align: center">Removed_variability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Lasso</td>
      <td style="text-align: center">0.4892</td>
      <td style="text-align: center">0.03054</td>
      <td style="text-align: center">None</td>
    </tr>
    <tr>
      <td style="text-align: center">Lasso adj</td>
      <td style="text-align: center">15.97</td>
      <td style="text-align: center">0.04037</td>
      <td style="text-align: center">sini, density</td>
    </tr>
    <tr>
      <td style="text-align: center">Normal regression</td>
      <td style="text-align: center">0.03432</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">None</td>
    </tr>
    <tr>
      <td style="text-align: center">Normal regression adj</td>
      <td style="text-align: center">17.08</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">sini, density</td>
    </tr>
    <tr>
      <td style="text-align: center">Ridge</td>
      <td style="text-align: center">0.5011</td>
      <td style="text-align: center">0.01</td>
      <td style="text-align: center">None</td>
    </tr>
    <tr>
      <td style="text-align: center">Ridge adj</td>
      <td style="text-align: center">16.34</td>
      <td style="text-align: center">0.01</td>
      <td style="text-align: center">sini, density</td>
    </tr>
  </tbody>
</table>

<p>For adj means without sini and density</p>

<p>As we can see from this table the regression without removing ‘sini’ and
density’ were almost perfect and with high probability false while the
other analysis seems more reliable. Basically using siri and density
inside the predictors means use something that results be the same as
the y so we do no use the other variables anymore but just that one.</p>

<p>To have a better idea we can also plot and see these results.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_tot</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">MSE</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="o">=</span><span class="n">reorder</span><span class="p">(</span><span class="n">Regression</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">MSE</span><span class="p">),</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="n">Regression</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_bar</span><span class="p">(</span><span class="n">stat</span><span class="o">=</span><span class="s1">'identity'</span><span class="p">)</span><span class="o">+</span><span class="w"> </span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">legend.position</span><span class="o">=</span><span class="s2">"bottom"</span><span class="p">,</span><span class="n">legend.title</span><span class="o">=</span><span class="n">element_blank</span><span class="p">())</span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"MSE of the differents model"</span><span class="p">)</span><span class="o">+</span><span class="w"> </span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-40-1.png" alt="" /><!-- --></p>

<blockquote>
  <blockquote>
    <h2 id="exercise-2">Exercise 2</h2>

    <p>In this question, you will revisit the Hitters dataset. The goal is
to predict the salary of baseball players, as a quantitative
variable, from the other explanatory variables.</p>
  </blockquote>
</blockquote>

<blockquote>
  <h3 id="1-split-the-data-into-traintest-set">1. Split the data into train/test set</h3>
</blockquote>

<p>We already know that all the NAs are all in the column Salary so we
directly drop it.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">na.omit</span><span class="p">(</span><span class="n">ISLR</span><span class="o">::</span><span class="n">Hitters</span><span class="p">)</span><span class="w">
</span><span class="n">r</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="w"> </span><span class="n">nrow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">*</span><span class="m">0.33</span><span class="p">)</span><span class="w">
</span><span class="n">train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">r</span><span class="p">,]</span><span class="w">
</span><span class="n">train_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="m">19</span><span class="p">]</span><span class="w">
</span><span class="n">test_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="m">-19</span><span class="p">]</span><span class="w">
</span><span class="n">test_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="m">19</span><span class="p">]</span><span class="w">
</span></code></pre></div></div>

<blockquote>
  <h3 id="2-fit-a-decision-tree-on-the-training-data-and-plot-the-results">2. Fit a decision tree on the training data and plot the results.</h3>
</blockquote>

<p>We create a the tree as:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">Tree1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tree</span><span class="p">(</span><span class="n">Salary</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">)</span><span class="w">
</span><span class="p">(</span><span class="n">summary</span><span class="p">(</span><span class="n">Tree1</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## 
## Regression tree:
## tree(formula = Salary ~ ., data = train)
## Variables actually used in tree construction:
## [1] "CRuns"   "CAtBat"  "Hits"    "CHmRun"  "PutOuts" "Years"   "CHits"  
## Number of terminal nodes:  10 
## Residual mean deviance:  65070 = 10870000 / 167 
## Distribution of residuals:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -785.40  -99.17  -24.17    0.00  125.80 1458.00
</code></pre></div></div>

<p>Seems that the model is quiet simple, we have only 10 terminal nodes,
the residual mean is 65070.</p>

<p>Let’s first see what our tree is structured and then represent it as a
tree:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">Tree1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## node), split, n, deviance, yval
##       * denotes terminal node
## 
##   1) root 177 41630000  558.1  
##     2) CRuns &lt; 288 94  7002000  282.1  
##       4) CAtBat &lt; 1257.5 61  4650000  192.7  
##         8) Hits &lt; 40.5 5  3163000  669.5 *
##         9) Hits &gt; 40.5 56   249400  150.2 *
##       5) CAtBat &gt; 1257.5 33   963200  447.4 *
##     3) CRuns &gt; 288 83 19350000  870.7  
##       6) Hits &lt; 122.5 37  1848000  569.0 *
##       7) Hits &gt; 122.5 46 11430000 1113.0  
##        14) CHmRun &lt; 256.5 39  6016000 1005.0  
##          28) PutOuts &lt; 1124.5 34  4101000  936.4  
##            56) Years &lt; 9.5 19  2422000 1087.0  
##             112) CHits &lt; 911 13   565100  926.0 *
##             113) CHits &gt; 911 6   794600 1435.0 *
##            57) Years &gt; 9.5 15   708900  746.3  
##             114) Years &lt; 11.5 5   112100  490.4 *
##             115) Years &gt; 11.5 10   105900  874.2 *
##          29) PutOuts &gt; 1124.5 5   678500 1469.0 *
##        15) CHmRun &gt; 256.5 7  2386000 1719.0 *
</code></pre></div></div>

<p>Basically this is telling us the structure of our tree. Starting from
the root we can see that we have 177 observation, at each row R is
telling us the criterion used to split the tree, the number of
observations that are left and how the deviance is dropped after the
splitting.</p>

<p>The first features used to split the tree are also the most
significative, so in this case the most important is CRuns followed by
Hits and CAtBat.</p>

<p>To be clear the deviance is basically the level of ‘impurity’ of the
tree that can be calculated usually with two methods: the Entropy or the
Gini index, both basically represent the level of homogeneity from that
node and below (starting from that, looking at the Tree in a recursive
way).</p>

<p>Let’s also plot the tree:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">RColorBrewer</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">rattle</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">rpart</span><span class="p">)</span><span class="w">
</span><span class="n">fancyRpartPlot</span><span class="p">(</span><span class="n">rpart</span><span class="p">(</span><span class="n">Tree1</span><span class="p">),</span><span class="n">yesno</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="n">split.col</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">nn.col</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="w"> 
               </span><span class="n">caption</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="n">palette</span><span class="o">=</span><span class="s2">"Paired"</span><span class="p">,</span><span class="n">branch.col</span><span class="o">=</span><span class="s2">"black"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-44-1.png" alt="" /><!-- --></p>

<blockquote>
  <h3 id="3-chose-the-good-trade-off-complexity---bias">3. Chose the good trade off complexity - bias</h3>
</blockquote>

<p>To perform this operation we basically prune the tree looking for a good
trade off between complexity and bias. Tree are very affected to the
problem of overfitting. To do so we will use cross validation and with a
certain parameter k (which penalize the number of terminal nodes) we
will search for the best choice of k in terms of deviance.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">pander</span><span class="p">)</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">cross_v_Tree1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.tree</span><span class="p">(</span><span class="n">object</span><span class="o">=</span><span class="n">Tree1</span><span class="p">,</span><span class="w"> </span><span class="n">FUN</span><span class="o">=</span><span class="n">prune.tree</span><span class="p">)</span><span class="w">
</span><span class="n">tmp</span><span class="o">&lt;-</span><span class="p">(</span><span class="n">data.frame</span><span class="p">(</span><span class="n">Size</span><span class="o">=</span><span class="n">cross_v_Tree1</span><span class="o">$</span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="o">=</span><span class="n">cross_v_Tree1</span><span class="o">$</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">Dev</span><span class="o">=</span><span class="n">cross_v_Tree1</span><span class="o">$</span><span class="n">dev</span><span class="p">))</span><span class="w">
</span><span class="n">pander</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Size</th>
      <th style="text-align: center">K</th>
      <th style="text-align: center">Dev</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">10</td>
      <td style="text-align: center">-Inf</td>
      <td style="text-align: center">28800868</td>
    </tr>
    <tr>
      <td style="text-align: center">9</td>
      <td style="text-align: center">490850</td>
      <td style="text-align: center">28646310</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: center">1016431</td>
      <td style="text-align: center">28183050</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: center">1236268</td>
      <td style="text-align: center">29125417</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">1237897</td>
      <td style="text-align: center">29125417</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">1388986</td>
      <td style="text-align: center">27836578</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">3025955</td>
      <td style="text-align: center">29704267</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">6077789</td>
      <td style="text-align: center">31253198</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">15269120</td>
      <td style="text-align: center">43179848</td>
    </tr>
  </tbody>
</table>

<p>From this we are interested in k, which represent the value of the
cost-complexity parameter, and in dev which is the deviance (as we saw
before the level of impurity).</p>

<p>Let’s visually see how changes the deviance according to the value of k
and according to the size (number of terminal nodes) of the tree. we
expect to see that higher k means higher deviance and on contrary higher
size implies lower deviance.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#we removed the first obs with k -inf for graphical purpose.</span><span class="w">
</span><span class="n">g1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">tmp</span><span class="p">[</span><span class="m">-1</span><span class="p">,],</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">Dev</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="o">=</span><span class="n">K</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">"royalblue"</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">"royalblue"</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"Dev ~ K"</span><span class="p">,</span><span class="w"> </span><span class="n">caption</span><span class="o">=</span><span class="s2">"obtained from cross validation"</span><span class="p">)</span><span class="w">
</span><span class="n">g2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">tmp</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">Dev</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="o">=</span><span class="n">Size</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">"firebrick"</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="o">+</span><span class="n">geom_point</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">"firebrick"</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="o">+</span><span class="w">
  </span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"Dev ~ Size"</span><span class="p">,</span><span class="n">caption</span><span class="o">=</span><span class="s2">"obtained from cross validation"</span><span class="p">)</span><span class="w">

</span><span class="n">grid.arrange</span><span class="p">(</span><span class="n">g1</span><span class="p">,</span><span class="n">g2</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-46-1.png" alt="" /><!-- --></p>

<p>Now from the cross-validation we look at the tree size (number of
terminal nodes) having the lowest deviance, prune the first tree that we
created and see how it performs on the test set.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">opti_size</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cross_v_Tree1</span><span class="o">$</span><span class="n">size</span><span class="p">[</span><span class="n">which.min</span><span class="p">(</span><span class="n">cross_v_Tree1</span><span class="o">$</span><span class="n">dev</span><span class="p">)]</span><span class="w">
</span><span class="n">pruned_Tree1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">prune.tree</span><span class="p">(</span><span class="n">Tree1</span><span class="p">,</span><span class="w"> </span><span class="n">best</span><span class="o">=</span><span class="n">opti_size</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">pruned_Tree1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## 
## Regression tree:
## snip.tree(tree = Tree1, nodes = c(14L, 2L))
## Variables actually used in tree construction:
## [1] "CRuns"  "Hits"   "CHmRun"
## Number of terminal nodes:  4 
## Residual mean deviance:  99730 = 17250000 / 173 
## Distribution of residuals:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -785.40 -182.10  -82.13    0.00  142.90 1845.00
</code></pre></div></div>

<p>As expected now we have only 4 terminal nodes and a deviance is higher.
Let’s quickly compare it with the deviance of the full tree computed
before:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"Deviance of the full tree:"</span><span class="p">,</span><span class="w"> </span><span class="n">summary</span><span class="p">(</span><span class="n">Tree1</span><span class="p">)</span><span class="o">$</span><span class="n">dev</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "Deviance of the full tree: 10865937.4203605"
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"Deviance of the pruned tree:"</span><span class="p">,</span><span class="w"> </span><span class="n">summary</span><span class="p">(</span><span class="n">pruned_Tree1</span><span class="p">)</span><span class="o">$</span><span class="n">dev</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "Deviance of the pruned tree: 17252799.8528633"
</code></pre></div></div>

<p>Let’s also plot this small tree:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">pruned_Tree1</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="n">pruned_Tree1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-49-1.png" alt="" /><!-- --></p>

<p>We can see that the most important variables used as criterion to spilt
the tree are: CRuns, Hits, CHmRun.</p>

<p>Let’s see how it perform on the test data:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">pre_Tree1</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">Tree1</span><span class="p">,</span><span class="w"> </span><span class="n">test_x</span><span class="p">)</span><span class="w">
</span><span class="n">pre_Pruned_Tree1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w">  </span><span class="n">predict</span><span class="p">(</span><span class="n">pruned_Tree1</span><span class="p">,</span><span class="w"> </span><span class="n">test_x</span><span class="p">)</span><span class="w">
</span><span class="n">MSE_Tree</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">((</span><span class="n">test_y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">pre_Tree1</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">MSE_Pruned_Tree</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">((</span><span class="n">test_y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">pre_Pruned_Tree1</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"MSE full tree: "</span><span class="p">,</span><span class="w"> </span><span class="n">MSE_Tree</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "MSE full tree:  75757.5419478215"
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"MSE Pruned tree: "</span><span class="p">,</span><span class="w"> </span><span class="n">MSE_Pruned_Tree</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "MSE Pruned tree:  79782.7325250855"
</code></pre></div></div>

<p>Since the MSE of the full tree is a few slower could be that we have low
variance inside our dataset.</p>

<blockquote>
  <h3 id="3-apply-bagging-on-the-training-portion-of-the-data-and-evaluate-the-test-mse-does-bagging-improve-the-performance">3. Apply bagging on the training portion of the data and evaluate the test MSE. Does bagging improve the performance?</h3>
</blockquote>

<p>To do so we basically perform an operation with the randomForest
package. To perform the bagging operation we set the number of allowed
parameter for tree equal to the all parameter. We can perform this
operation of bagging directly using that package as:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">randomForest</span><span class="p">)</span><span class="w">
</span><span class="n">Tree1_bag</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">randomForest</span><span class="p">(</span><span class="n">Salary</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">mtry</span><span class="o">=</span><span class="nf">length</span><span class="p">(</span><span class="n">train</span><span class="p">)</span><span class="m">-1</span><span class="p">,</span><span class="w"> </span><span class="n">ntree</span><span class="o">=</span><span class="m">200</span><span class="p">)</span><span class="w">
</span><span class="n">pred</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">Tree1_bag</span><span class="p">,</span><span class="w"> </span><span class="n">test_x</span><span class="p">)</span><span class="w">
</span><span class="n">MSE_bagged</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">((</span><span class="n">test_y</span><span class="o">-</span><span class="n">pred</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">MSE_bagged</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] 48477.79
</code></pre></div></div>

<p>The performance is definitely better.</p>

<blockquote>
  <h3 id="4-when-we-grow-a-random-forest-we-have-to-choose-the-number-m-of-variables-to-consider-at-each-split-set-the-range-for-m-from-1-to-nvar-define-a-matrix-with-nvar-rows-and-2-columns-and-fill-it-with-the-test-error-and-oob-error-on-training-data-corresponding-to-each-choice-of-m-save-the-matrix-as-a-dataframe-and-give-it-suitable-column-names-compare-oob-errors-with-test-errors-across-the-m-values-are-the-values-different-do-they-reach-the-minimum-for-the-same-value-of-m">4. When we grow a random forest, we have to choose the number m of variables to consider at each split. Set the range for m from 1 to nvar. Define a matrix with nvar rows and 2 columns and fill it with the test error and OOB error on training data corresponding to each choice of m. Save the matrix as a dataframe and give it suitable column names. Compare OOB errors with test errors across the m values. Are the values different? Do they reach the minimum for the same value of m?</h3>
</blockquote>

<p>So we will fit 19 different random forest each one having a different
number of parameter that correspond to the ‘m’ number of variable to
taking in count when perform the random split of them. Basically our
algorithm at each step, will subset randomly the features and pick m
random feature from them to build the tree.</p>

<p>So now we are collecting for each possible number of m from 1 to the
total number of features in the data set the training error and the test
error. We will use the default number of tree for a randomForest which
is 500.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">oob_e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
</span><span class="n">training_e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
</span><span class="n">test_e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="nf">length</span><span class="p">(</span><span class="n">test_x</span><span class="p">)){</span><span class="w">
  </span><span class="n">forest</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">randomForest</span><span class="p">(</span><span class="n">Salary</span><span class="o">~</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">ntree</span><span class="o">=</span><span class="m">500</span><span class="p">,</span><span class="w"> </span><span class="n">mtry</span><span class="o">=</span><span class="n">i</span><span class="p">)</span><span class="w">
  </span><span class="n">oob_e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">append</span><span class="p">(</span><span class="n">oob_e</span><span class="p">,</span><span class="n">forest</span><span class="o">$</span><span class="n">mse</span><span class="p">[</span><span class="m">500</span><span class="p">])</span><span class="w"> </span><span class="c1"># not sure </span><span class="w">
  </span><span class="n">pred</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">forest</span><span class="p">,</span><span class="w"> </span><span class="n">test_x</span><span class="p">)</span><span class="w">
  </span><span class="n">MSE</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">((</span><span class="n">pred</span><span class="o">-</span><span class="n">test_y</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
  </span><span class="n">test_e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">append</span><span class="p">(</span><span class="n">test_e</span><span class="p">,</span><span class="w"> </span><span class="n">MSE</span><span class="p">)</span><span class="w">
  </span><span class="n">pred_t</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">forest</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="p">)</span><span class="w">
  </span><span class="n">MSE_t</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">((</span><span class="n">pred_t</span><span class="o">-</span><span class="n">train_y</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
  </span><span class="n">training_e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">append</span><span class="p">(</span><span class="n">training_e</span><span class="p">,</span><span class="w"> </span><span class="n">MSE_t</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Now we plot the test error and the training error over all the possible
m, from 1 to m.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">check_perfomance</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">M</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="nf">length</span><span class="p">(</span><span class="n">test_x</span><span class="p">),</span><span class="w"> 
                               </span><span class="n">Test_e</span><span class="o">=</span><span class="n">test_e</span><span class="p">,</span><span class="w"> </span><span class="n">Training_e</span><span class="o">=</span><span class="n">training_e</span><span class="p">,</span><span class="w">
                               </span><span class="n">OOB_e</span><span class="w"> </span><span class="o">=</span><span class="n">oob_e</span><span class="w"> </span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">check_perfomance</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">M</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">check_perfomance</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">Test_e</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"Test Error"</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">check_perfomance</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">Test_e</span><span class="p">),</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"red"</span><span class="p">)</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">check_perfomance</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">Training_e</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"Training Error"</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">check_perfomance</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">Training_e</span><span class="p">),</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"blue"</span><span class="p">)</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">check_perfomance</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">OOB_e</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"OOB Error"</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">check_perfomance</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">OOB_e</span><span class="p">),</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"gold"</span><span class="p">)</span><span class="o">+</span><span class="w">
  </span><span class="n">scale_colour_manual</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">values</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"gold2"</span><span class="p">,</span><span class="s2">"red"</span><span class="p">,</span><span class="s2">"blue"</span><span class="p">))</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'Training, OOB and error rate ~ M'</span><span class="p">)</span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s2">"Training and Error rate"</span><span class="p">)</span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">legend.position</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"top"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-53-1.png" alt="" /><!-- --></p>

<p>Hard to define the good choice of M. We see that reached m=5 the test
error is quiet regular while the training error also remains almost
fixed. i think that we can choice an M around 7 which is what is
recommend when we are dealing regression contest (p/3).</p>

<blockquote>
  <h3 id="5-reach-a-conclusion-about-the-optimal-random-forest-model-on-the-training-data-and-evaluate-the-model-performance-on-the-test-data-identify-the-variables-that-are-important-for-prediction">5. Reach a conclusion about the optimal random forest model on the training data and evaluate the model performance on the test data. Identify the variables that are important for prediction</h3>
</blockquote>

<p>Since we decide to use the model with a M=7 we test this again this
model on the test data and we also try to understand what are the most
important variables. this last operation is made thanks to an argument
inserted into the randomForest function that allows to keep track of the
importance of the all features.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">forest</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">randomForest</span><span class="p">(</span><span class="n">Salary</span><span class="o">~</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">ntree</span><span class="o">=</span><span class="m">500</span><span class="p">,</span><span class="w"> </span><span class="n">mtry</span><span class="o">=</span><span class="m">7</span><span class="p">,</span><span class="w"> </span><span class="n">importance</span><span class="o">=</span><span class="nb">T</span><span class="p">)</span><span class="w">
</span><span class="n">MSE_BEST</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">((</span><span class="n">predict</span><span class="p">(</span><span class="n">forest</span><span class="p">,</span><span class="n">test_x</span><span class="p">)</span><span class="o">-</span><span class="n">test_y</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"The MSE over the Test set using m=7 is:"</span><span class="p">,</span><span class="w"> </span><span class="n">MSE_BEST</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "The MSE over the Test set using m=7 is: 46856.3369010197"
</code></pre></div></div>

<p>While the important features used from the forest are:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">forest</span><span class="o">$</span><span class="n">importance</span><span class="p">)</span><span class="w">
</span><span class="n">tmp</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">order</span><span class="p">(</span><span class="n">x</span><span class="o">$</span><span class="n">IncNodePurity</span><span class="p">,</span><span class="w"> </span><span class="n">decreasing</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">T</span><span class="p">),]</span><span class="w">
</span><span class="n">pander</span><span class="p">((</span><span class="n">tmp</span><span class="p">),</span><span class="w"> </span><span class="n">caption</span><span class="o">=</span><span class="s2">"Forest with M=7"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">X.IncMSE</th>
      <th style="text-align: center">IncNodePurity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>CHits</strong></td>
      <td style="text-align: center">39019</td>
      <td style="text-align: center">4641070</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>CRBI</strong></td>
      <td style="text-align: center">26186</td>
      <td style="text-align: center">4575800</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>CRuns</strong></td>
      <td style="text-align: center">36984</td>
      <td style="text-align: center">4559366</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Hits</strong></td>
      <td style="text-align: center">10860</td>
      <td style="text-align: center">3196894</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>CAtBat</strong></td>
      <td style="text-align: center">25101</td>
      <td style="text-align: center">3140547</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>CWalks</strong></td>
      <td style="text-align: center">22004</td>
      <td style="text-align: center">2930865</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Walks</strong></td>
      <td style="text-align: center">4426</td>
      <td style="text-align: center">2552785</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>RBI</strong></td>
      <td style="text-align: center">6848</td>
      <td style="text-align: center">2516381</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Runs</strong></td>
      <td style="text-align: center">8137</td>
      <td style="text-align: center">2357556</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>CHmRun</strong></td>
      <td style="text-align: center">11971</td>
      <td style="text-align: center">2340936</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>PutOuts</strong></td>
      <td style="text-align: center">4642</td>
      <td style="text-align: center">2011789</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>AtBat</strong></td>
      <td style="text-align: center">12102</td>
      <td style="text-align: center">1979790</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>HmRun</strong></td>
      <td style="text-align: center">3341</td>
      <td style="text-align: center">907940</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Years</strong></td>
      <td style="text-align: center">11051</td>
      <td style="text-align: center">887128</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Assists</strong></td>
      <td style="text-align: center">-949.5</td>
      <td style="text-align: center">585313</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Errors</strong></td>
      <td style="text-align: center">-340.1</td>
      <td style="text-align: center">500344</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>League</strong></td>
      <td style="text-align: center">-903.7</td>
      <td style="text-align: center">101676</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>NewLeague</strong></td>
      <td style="text-align: center">60.78</td>
      <td style="text-align: center">89925</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Division</strong></td>
      <td style="text-align: center">-43.81</td>
      <td style="text-align: center">48158</td>
    </tr>
  </tbody>
</table>

<p>Forest with M=7</p>

<p>We are interested in second column which represent the total decrease in
node impurity thanks to that variable (higher is better). we can see
that the 5 more importants are CRBI, CRuns, CHits, Hits, Cwalks.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
</span><span class="n">important_feature_plot</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.data.frame</span><span class="p">(</span><span class="n">varImpPlot</span><span class="p">(</span><span class="n">forest</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-56-1.png" alt="" /><!-- --></p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">important_feature_plot</span><span class="o">$</span><span class="n">varnames</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rownames</span><span class="p">(</span><span class="n">important_feature_plot</span><span class="p">)</span><span class="w">
</span><span class="n">important_feature_plot</span><span class="o">$</span><span class="n">var_categ</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">19</span><span class="p">)</span><span class="w"> 
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ggplot</span><span class="p">(</span><span class="n">important_feature_plot</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">reorder</span><span class="p">(</span><span class="n">varnames</span><span class="p">,</span><span class="w"> </span><span class="n">IncNodePurity</span><span class="p">),</span><span class="w"> 
                                   </span><span class="n">y</span><span class="o">=</span><span class="n">IncNodePurity</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="o">=</span><span class="n">as.factor</span><span class="p">(</span><span class="n">var_categ</span><span class="p">)))</span><span class="w"> </span><span class="o">+</span><span class="w"> 
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_segment</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">varnames</span><span class="p">,</span><span class="n">xend</span><span class="o">=</span><span class="n">varnames</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="n">yend</span><span class="o">=</span><span class="n">IncNodePurity</span><span class="p">),</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_color_discrete</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"Variable Group"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s2">"IncNodePurity"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="o">+</span><span class="n">coord_flip</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> 
  </span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w"> </span><span class="n">theme</span><span class="p">(</span><span class="n">legend.position</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"None"</span><span class="p">)</span><span class="o">+</span><span class="w"> 
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"Importance in terms of NodePurity\n"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-57-1.png" alt="" /><!-- --></p>

<blockquote>
  <h3 id="6-fit-a-regression-tree-on-the-training-data-using-boosting-find-the-optimal-number-of-boosting-iterations-both-by-evaluating-the-oob-error-and-the-cross-validation-error-produce-plots-with-oob-error-and-cv-error-against-the-number-of-iterations-are-the-two-methods-leading-to-the-same-choice-of-the-optimal-number-of-iterations-reach-a-conclusion-about-the-optimal-model-evaluate-the-test-mse-of-this-model-and-produce-a-partial-dependence-plot-of-the-resulting-top-n-variables-n-of-your-choice">6. Fit a regression tree on the training data using boosting. Find the optimal number of boosting iterations, both by evaluating the OOB error and the cross-validation error. Produce plots with OOB error and CV error against the number of iterations: are the two methods leading to the same choice of the optimal number of iterations? Reach a conclusion about the optimal model, evaluate the test MSE of this model and produce a partial dependence plot of the resulting top N variables (N of your choice).</h3>
</blockquote>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">gbm</span><span class="p">)</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">forest_boosted</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gbm</span><span class="p">(</span><span class="n">Salary</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">distribution</span><span class="o">=</span><span class="s2">"gaussian"</span><span class="p">,</span><span class="w">
</span><span class="n">n.trees</span><span class="o">=</span><span class="m">5000</span><span class="p">,</span><span class="w"> </span><span class="n">interaction.depth</span><span class="o">=</span><span class="m">4</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Since we are working in the regression context we must specify the
distribution equal to ‘gaussian’, the other important argument is
interaction.depth. Basically the boosted algorithm create a very large
of tree but with a imitated number of terminal nodes, that
interaction.depth represent exactly the maximum number of nodes allowed.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">summary</span><span class="p">(</span><span class="n">forest_boosted</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-59-1.png" alt="" /><!-- --></p>

<p>bag.fraction from documentation: the fraction of the training set
observations randomly selected to propose the next tree in the
expansion. This introduces randomnesses into the model fit. If
bag.fraction &lt; 1 then running the same model twice will result in
similar but different fits. gbm uses the R random number generator so
set.seed can ensure that the model can be reconstructed.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">training_error</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
</span><span class="n">outofbagerror</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
</span><span class="n">cverror</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
</span><span class="n">noftrees</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">500</span><span class="w"> </span><span class="c1"># since it is very slow, so it easy to modify it.</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">20</span><span class="p">){</span><span class="w">
  </span><span class="n">bosted_tmp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gbm</span><span class="p">(</span><span class="n">Salary</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">distribution</span><span class="o">=</span><span class="s2">"gaussian"</span><span class="p">,</span><span class="w">
               </span><span class="n">n.trees</span><span class="o">=</span><span class="n">noftrees</span><span class="p">,</span><span class="w"> </span><span class="n">interaction.depth</span><span class="o">=</span><span class="n">i</span><span class="w"> </span><span class="p">,</span><span class="n">cv.folds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w">
  </span><span class="c1"># not sure OOB:</span><span class="w">
  </span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="n">nrow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">*</span><span class="m">0.5</span><span class="p">)</span><span class="w">
  </span><span class="n">pred_oob</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">bosted_tmp</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">r</span><span class="p">,])</span><span class="w">
  </span><span class="n">out_e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">((</span><span class="n">pred_oob</span><span class="o">-</span><span class="n">data</span><span class="o">$</span><span class="n">Salary</span><span class="p">[</span><span class="o">-</span><span class="n">r</span><span class="p">])</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
  </span><span class="n">outofbagerror</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">append</span><span class="p">(</span><span class="n">outofbagerror</span><span class="p">,</span><span class="w"> </span><span class="n">out_e</span><span class="p">)</span><span class="w">

  </span><span class="n">training_error</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">append</span><span class="p">(</span><span class="n">training_error</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">bosted_tmp</span><span class="o">$</span><span class="n">train.error</span><span class="p">))</span><span class="w">
  </span><span class="n">cverror</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">append</span><span class="p">(</span><span class="n">cverror</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">bosted_tmp</span><span class="o">$</span><span class="n">cv.error</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Let’s see the results of our analysis:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gbm</span><span class="o">::</span><span class="n">gbm.perf</span><span class="p">(</span><span class="n">bosted_tmp</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"OOB"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-61-1.png" alt="" /><!-- --></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] 20
## attr(,"smoother")
## Call:
## loess(formula = object$oobag.improve ~ x, enp.target = min(max(4, 
##     length(x)/10), 50))
## 
## Number of Observations: 500 
## Equivalent Number of Parameters: 39.85 
## Residual Standard Error: 325.4
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tmpdf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">Iteration</span><span class="o">=</span><span class="m">1</span><span class="o">:</span><span class="m">20</span><span class="p">,</span><span class="n">OOB_error</span><span class="o">=</span><span class="n">outofbagerror</span><span class="p">,</span><span class="w"> </span><span class="n">CV_error</span><span class="o">=</span><span class="n">cverror</span><span class="p">,</span><span class="w"> 
                    </span><span class="n">Train_error</span><span class="o">=</span><span class="n">training_error</span><span class="p">)</span><span class="w">
</span><span class="n">pander</span><span class="p">(</span><span class="n">tmpdf</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Iteration</th>
      <th style="text-align: center">OOB_error</th>
      <th style="text-align: center">CV_error</th>
      <th style="text-align: center">Train_error</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">55385</td>
      <td style="text-align: center">92847</td>
      <td style="text-align: center">50153</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">36137</td>
      <td style="text-align: center">86799</td>
      <td style="text-align: center">31175</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">45038</td>
      <td style="text-align: center">88257</td>
      <td style="text-align: center">22266</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">54888</td>
      <td style="text-align: center">95575</td>
      <td style="text-align: center">18127</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">34856</td>
      <td style="text-align: center">87375</td>
      <td style="text-align: center">16602</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: center">21598</td>
      <td style="text-align: center">84029</td>
      <td style="text-align: center">16318</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: center">50589</td>
      <td style="text-align: center">92686</td>
      <td style="text-align: center">14233</td>
    </tr>
    <tr>
      <td style="text-align: center">8</td>
      <td style="text-align: center">22686</td>
      <td style="text-align: center">88158</td>
      <td style="text-align: center">14727</td>
    </tr>
    <tr>
      <td style="text-align: center">9</td>
      <td style="text-align: center">32194</td>
      <td style="text-align: center">84644</td>
      <td style="text-align: center">14727</td>
    </tr>
    <tr>
      <td style="text-align: center">10</td>
      <td style="text-align: center">20348</td>
      <td style="text-align: center">101439</td>
      <td style="text-align: center">13852</td>
    </tr>
    <tr>
      <td style="text-align: center">11</td>
      <td style="text-align: center">21316</td>
      <td style="text-align: center">88437</td>
      <td style="text-align: center">15200</td>
    </tr>
    <tr>
      <td style="text-align: center">12</td>
      <td style="text-align: center">36377</td>
      <td style="text-align: center">90824</td>
      <td style="text-align: center">14732</td>
    </tr>
    <tr>
      <td style="text-align: center">13</td>
      <td style="text-align: center">23258</td>
      <td style="text-align: center">92980</td>
      <td style="text-align: center">14679</td>
    </tr>
    <tr>
      <td style="text-align: center">14</td>
      <td style="text-align: center">32189</td>
      <td style="text-align: center">92937</td>
      <td style="text-align: center">14049</td>
    </tr>
    <tr>
      <td style="text-align: center">15</td>
      <td style="text-align: center">27228</td>
      <td style="text-align: center">89708</td>
      <td style="text-align: center">14988</td>
    </tr>
    <tr>
      <td style="text-align: center">16</td>
      <td style="text-align: center">31590</td>
      <td style="text-align: center">91493</td>
      <td style="text-align: center">14077</td>
    </tr>
    <tr>
      <td style="text-align: center">17</td>
      <td style="text-align: center">16919</td>
      <td style="text-align: center">89900</td>
      <td style="text-align: center">14205</td>
    </tr>
    <tr>
      <td style="text-align: center">18</td>
      <td style="text-align: center">39235</td>
      <td style="text-align: center">90232</td>
      <td style="text-align: center">15248</td>
    </tr>
    <tr>
      <td style="text-align: center">19</td>
      <td style="text-align: center">19244</td>
      <td style="text-align: center">94167</td>
      <td style="text-align: center">15172</td>
    </tr>
    <tr>
      <td style="text-align: center">20</td>
      <td style="text-align: center">23728</td>
      <td style="text-align: center">85323</td>
      <td style="text-align: center">13348</td>
    </tr>
  </tbody>
</table>

<p>To have a better understanding we can also plot it as well.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">title_1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="s2">"Trainig, Cross-validation &amp; Out Of Bag error ~ Number of terminal nodes"</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">tmpdf</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">Iteration</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">tmpdf</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">OOB_error</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"OOB Error"</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">tmpdf</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">OOB_error</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"OOB Error"</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">tmpdf</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">Train_error</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"Train_error"</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">tmpdf</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">Train_error</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"Train_error"</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">tmpdf</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">CV_error</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"CV error"</span><span class="p">))</span><span class="o">+</span><span class="w"> 
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">tmpdf</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">CV_error</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="o">=</span><span class="s2">"CV error"</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">scale_colour_manual</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">values</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"red"</span><span class="p">,</span><span class="s2">"blue"</span><span class="p">,</span><span class="s2">"gold2"</span><span class="p">))</span><span class="o">+</span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">legend.position</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"top"</span><span class="p">)</span><span class="o">+</span><span class="n">labs</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">title_1</span><span class="p">,</span><span class="w">
                                      </span><span class="n">caption</span><span class="o">=</span><span class="n">paste</span><span class="p">(</span><span class="s2">"Number of tree ="</span><span class="p">,</span><span class="n">noftrees</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-63-1.png" alt="" /><!-- --> The methods seems
that leads a different choice of the number of iteration. the cross
validation set seem very irregular and seems that it does not converge
in any case while the OOB seems that start converge around 11 iteration.
So we pick 10 as optimal number of iteration.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Optimized</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gbm</span><span class="p">(</span><span class="n">Salary</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">distribution</span><span class="o">=</span><span class="s2">"gaussian"</span><span class="p">,</span><span class="w">
               </span><span class="n">n.trees</span><span class="o">=</span><span class="n">noftrees</span><span class="p">,</span><span class="w"> </span><span class="n">interaction.depth</span><span class="o">=</span><span class="m">11</span><span class="w"> </span><span class="p">,</span><span class="n">cv.folds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w">

</span><span class="n">pred_Op</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">Optimized</span><span class="p">,</span><span class="w"> </span><span class="n">test_x</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Using 93 trees...
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mse_boost</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">((</span><span class="n">pred_Op</span><span class="o">-</span><span class="n">test_y</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">print</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="s2">"MSE of optimized boosted forest:"</span><span class="w"> </span><span class="p">,</span><span class="n">mse_boost</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "MSE of optimized boosted forest: 59354.0488668846"
</code></pre></div></div>

<p>Let’s plot the partial depend plot, trough that we are able to see how
much the variable affect the entire forest! so we are looking how much
important are the variable in question. we pick an arbitrary number of 4
variables:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">most_important_f</span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"CRBI"</span><span class="p">,</span><span class="s2">"Hits"</span><span class="p">,</span><span class="s2">"CHits"</span><span class="p">,</span><span class="s2">"CHits"</span><span class="p">)</span><span class="w">
</span><span class="n">plots1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">plot</span><span class="p">(</span><span class="n">Optimized</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="n">most_important_f</span><span class="p">[</span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="n">most_important_f</span><span class="p">[</span><span class="m">1</span><span class="p">])</span><span class="w">
</span><span class="n">plots2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">plot</span><span class="p">(</span><span class="n">Optimized</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="n">most_important_f</span><span class="p">[</span><span class="m">2</span><span class="p">],</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="n">most_important_f</span><span class="p">[</span><span class="m">2</span><span class="p">])</span><span class="w">
</span><span class="n">plots3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">plot</span><span class="p">(</span><span class="n">Optimized</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="n">most_important_f</span><span class="p">[</span><span class="m">3</span><span class="p">],</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="n">most_important_f</span><span class="p">[</span><span class="m">3</span><span class="p">])</span><span class="w">
</span><span class="n">plots4</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">plot</span><span class="p">(</span><span class="n">Optimized</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="n">most_important_f</span><span class="p">[</span><span class="m">4</span><span class="p">],</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="n">most_important_f</span><span class="p">[</span><span class="m">4</span><span class="p">])</span><span class="w">
</span><span class="n">grid.arrange</span><span class="p">(</span><span class="n">plots1</span><span class="p">,</span><span class="n">plots2</span><span class="p">,</span><span class="n">plots3</span><span class="p">,</span><span class="n">plots4</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-66-1.png" alt="" /><!-- --></p>

<blockquote>
  <h4 id="7-draw-some-general-conclusions-about-the-analysis-and-the-different-methods-that-you-considered">7. Draw some general conclusions about the analysis and the different methods that you considered</h4>
</blockquote>

<p>We saw different approach to the problems, now we can say something
about the models and which one could be the best. Basically the single
tree is a weak option, it cannot reach optimal level of accuracy while
using randomForest yes. Moreover if we can combine gradient boost
algorithm to randomForest the result will generally improve. Since with
those methods the risk of overfitting is quiet large we can also keep in
mind to eventually prune the trees or even better when performing
boosting operation try to keep the number of iteration slow so to let
the complexity of each tree small. To see which one perfomed better in
our dataframe we can compare the MSE!</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">final</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">MSE</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="n">MSE_Tree</span><span class="p">,</span><span class="n">MSE_Pruned_Tree</span><span class="p">,</span><span class="n">MSE_bagged</span><span class="p">,</span><span class="n">MSE_BEST</span><span class="p">,</span><span class="n">mse_boost</span><span class="p">),</span><span class="w"> 
                    </span><span class="n">Model</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s1">'Full tree'</span><span class="p">,</span><span class="s1">'Pruned tree'</span><span class="p">,</span><span class="s1">'Bagged tree'</span><span class="p">,</span><span class="w"> 
                            </span><span class="s1">'RandomForest'</span><span class="p">,</span><span class="s1">'RandomForest boosted'</span><span class="p">))</span><span class="w">
</span><span class="n">pander</span><span class="p">(</span><span class="n">final</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MSE</th>
      <th style="text-align: center">Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">75758</td>
      <td style="text-align: center">Full tree</td>
    </tr>
    <tr>
      <td style="text-align: center">79783</td>
      <td style="text-align: center">Pruned tree</td>
    </tr>
    <tr>
      <td style="text-align: center">48478</td>
      <td style="text-align: center">Bagged tree</td>
    </tr>
    <tr>
      <td style="text-align: center">46856</td>
      <td style="text-align: center">RandomForest</td>
    </tr>
    <tr>
      <td style="text-align: center">59354</td>
      <td style="text-align: center">RandomForest boosted</td>
    </tr>
  </tbody>
</table>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">final</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">final</span><span class="p">[</span><span class="n">order</span><span class="p">(</span><span class="n">final</span><span class="p">[</span><span class="s2">"MSE"</span><span class="p">]),]</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">final</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">MSE</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="o">=</span><span class="n">reorder</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">MSE</span><span class="p">),</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="n">Model</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_bar</span><span class="p">(</span><span class="n">stat</span><span class="o">=</span><span class="s1">'identity'</span><span class="p">)</span><span class="o">+</span><span class="w"> </span><span class="n">theme_tufte</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">legend.position</span><span class="o">=</span><span class="s2">"bottom"</span><span class="p">,</span><span class="n">legend.title</span><span class="o">=</span><span class="n">element_blank</span><span class="p">())</span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"MSE of the differents model"</span><span class="p">)</span><span class="o">+</span><span class="w"> </span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/home/assets/images/README_figs/shrinkage-models/README-unnamed-chunk-68-1.png" alt="" /><!-- --></p>
:ET